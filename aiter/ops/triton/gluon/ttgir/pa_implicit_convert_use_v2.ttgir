#blocked = #ttg.blocked<{sizePerThread = [1, 1, 1, 8], threadsPerWarp = [1, 4, 16, 1], warpsPerCTA = [4, 1, 1, 1], order = [3, 2, 1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 8], threadsPerWarp = [4, 16], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [64], warpsPerCTA = [4], order = [0]}>
// #blocked3 = #ttg.blocked<{sizePerThread = [1, 1, 8], threadsPerWarp = [1, 32, 2], warpsPerCTA = [1, 4, 1], order = [2, 1, 0]}>
#linear = #ttg.linear<{register = [[0, 1], [0, 2], [4, 0], [8, 0]], lane = [[0, 0], [0, 0], [0, 0], [0, 0], [0, 4], [0, 8]], warp = [[1, 0], [2, 0]], block = []}>
#linear1 = #ttg.linear<{register = [[0, 0, 0, 1], [0, 0, 0, 2], [0, 2, 0, 0], [0, 4, 0, 0], [0, 8, 0, 0], [4, 0, 0, 0], [8, 0, 0, 0]], lane = [[0, 0, 1, 0], [0, 0, 2, 0], [0, 0, 4, 0], [0, 0, 8, 0], [0, 0, 0, 4], [0, 1, 0, 0]], warp = [[1, 0, 0, 0], [2, 0, 0, 0]], block = []}>
// #linear2 = #ttg.linear<{register = [[0, 1, 0, 0], [0, 2, 0, 0], [2, 0, 0, 0], [4, 0, 0, 0], [8, 0, 0, 0], [0, 0, 4, 0], [0, 0, 8, 0]], lane = [[0, 0, 0, 1], [0, 0, 0, 2], [0, 0, 0, 4], [0, 0, 0, 8], [0, 4, 0, 0], [1, 0, 0, 0]], warp = [[0, 0, 1, 0], [0, 0, 2, 0]], block = []}>
// #linear3 = #ttg.linear<{register = [[0, 0, 1], [0, 0, 2], [0, 0, 4], [2, 0, 0], [4, 0, 0], [8, 0, 0], [0, 64, 0]], lane = [[0, 1, 0], [0, 2, 0], [0, 4, 0], [0, 8, 0], [0, 0, 8], [1, 0, 0]], warp = [[0, 16, 0], [0, 32, 0]], block = []}>
// #linear4 = #ttg.linear<{register = [[0, 1, 0], [0, 2, 0], [0, 4, 0], [2, 0, 0], [4, 0, 0], [8, 0, 0], [0, 0, 64]], lane = [[0, 0, 1], [0, 0, 2], [0, 0, 4], [0, 0, 8], [0, 8, 0], [1, 0, 0]], warp = [[0, 0, 16], [0, 0, 32]], block = []}>

// Q layout
#linear4 = #ttg.linear<{register = [[0, 1], [0, 2], [0, 4], [0, 32], [0, 64]], lane = [[1, 0], [2, 0], [4, 0], [8, 0], [0, 8], [0, 16]], warp = [[0, 0], [0, 0]], block = []}>
// K layout
#linear2 = #ttg.linear<{register = [[0, 1, 0, 0], [0, 2, 0, 0], [0, 4, 0, 0], [4, 0, 0, 0], [8, 0, 0, 0], [0, 0, 4, 0], [0, 0, 8, 0]], lane = [[0, 0, 0, 1], [0, 0, 0, 2], [0, 0, 0, 4], [0, 0, 0, 8], [1, 0, 0, 0], [2, 0, 0, 0]], warp = [[0, 0, 1, 0], [0, 0, 2, 0]], block = []}>
// V layout
#blocked3 = #ttg.linear<{register = [[0, 0, 1], [0, 0, 2], [0, 0, 4], [0, 0, 8], [4, 0, 0], [8, 0, 0], [0, 64, 0]], lane = [[0, 1, 0], [0, 2, 0], [0, 4, 0], [0, 8, 0], [1, 0, 0], [2, 0, 0]], warp = [[0, 16, 0], [0, 32, 0]], block = []}>
#linear6  = #ttg.linear<{register = [[0, 1, 0], [0, 2, 0], [0, 4, 0], [0, 8, 0], [4, 0, 0], [8, 0, 0], [0, 0, 64]], lane = [[0, 0, 1], [0, 0, 2], [0, 0, 4], [0, 0, 8], [1, 0, 0], [2, 0, 0]], warp = [[0, 0, 16], [0, 0, 32]], block = []}>
// P layout
#linear5 = #ttg.linear<{register = [[0, 1], [0, 2], [0, 4], [0, 8], [0, 64], [0, 128]], lane = [[1, 0], [2, 0], [4, 0], [8, 0], [0, 16], [0, 32]], warp = [[0, 0], [0, 0]], block = []}>

#loc = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":1862:0)
#loc1 = loc(unknown)
#loc52 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":1993:27)
#loc60 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":1997:21)
#mma = #ttg.amd_mfma<{version = 3, warpsPerCTA = [1, 4], instrShape = [16, 16], isTransposed = true}>
#shared = #ttg.swizzled_shared<{vec = 4, perPhase = 1, maxPhase = 16, order = [1, 0]}>
#shared1 = #ttg.swizzled_shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [0, 1]}>
#smem = #ttg.shared_memory
#loc87 = loc("exp_sums_ptr"(#loc))
#loc88 = loc("max_logits_ptr"(#loc))
#loc89 = loc("logits_ptr"(#loc))
#loc90 = loc("q_ptr"(#loc))
#loc91 = loc("k_cache_ptr"(#loc))
#loc92 = loc("v_cache_ptr"(#loc))
#loc93 = loc("blk_tables_ptrs"(#loc))
#loc94 = loc("seq_lens_ptr"(#loc))
#loc95 = loc("scale"(#loc))
#loc96 = loc("k_scale"(#loc))
#loc97 = loc("v_scale"(#loc))
#loc98 = loc("stride_max_logits_s"(#loc))
#loc99 = loc("stride_max_logits_nh"(#loc))
#loc100 = loc("stride_max_logits_p"(#loc))
#loc101 = loc("stride_logits_s"(#loc))
#loc102 = loc("stride_logits_nh"(#loc))
#loc103 = loc("stride_logits_p"(#loc))
#loc104 = loc("stride_logits_g"(#loc))
#loc105 = loc("stride_q_s"(#loc))
#loc106 = loc("stride_q_nh"(#loc))
#loc107 = loc("stride_k_b"(#loc))
#loc108 = loc("stride_k_nh"(#loc))
#loc109 = loc("stride_k_hz"(#loc))
#loc110 = loc("stride_k_bz"(#loc))
#loc111 = loc("stride_v_b"(#loc))
#loc112 = loc("stride_v_nh"(#loc))
#loc113 = loc("stride_v_hz"(#loc))
#loc114 = loc("stride_bt_s"(#loc))
#loc160 = loc("max_logit_new"(#loc52))
#loc166 = loc("exp_sum"(#loc60))
#loc188 = loc(callsite(#loc1 at #loc160))
#loc190 = loc(callsite(#loc1 at #loc166))
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.target = "hip:gfx942", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @_paged_attn_decode_v2_w_dot_kernel_reshape_noloop_qk(%exp_sums_ptr: !tt.ptr<f32> {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32} loc("exp_sums_ptr"(#loc)), %max_logits_ptr: !tt.ptr<f32> {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32} loc("max_logits_ptr"(#loc)), %logits_ptr: !tt.ptr<bf16> {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32} loc("logits_ptr"(#loc)), %q_ptr: !tt.ptr<bf16> {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32} loc("q_ptr"(#loc)), %k_cache_ptr: !tt.ptr<bf16> {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32} loc("k_cache_ptr"(#loc)), %v_cache_ptr: !tt.ptr<bf16> {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32} loc("v_cache_ptr"(#loc)), %blk_tables_ptrs: !tt.ptr<i32> {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32} loc("blk_tables_ptrs"(#loc)), %seq_lens_ptr: !tt.ptr<i32> {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32} loc("seq_lens_ptr"(#loc)), %scale: f32 loc("scale"(#loc)), %k_scale: f32 loc("k_scale"(#loc)), %v_scale: f32 loc("v_scale"(#loc)), %stride_max_logits_s: i32 {tt.divisibility = 16 : i32} loc("stride_max_logits_s"(#loc)), %stride_max_logits_nh: i32 {tt.divisibility = 16 : i32} loc("stride_max_logits_nh"(#loc)), %stride_max_logits_p: i32 loc("stride_max_logits_p"(#loc)), %stride_logits_s: i32 {tt.divisibility = 16 : i32} loc("stride_logits_s"(#loc)), %stride_logits_nh: i32 {tt.divisibility = 16 : i32} loc("stride_logits_nh"(#loc)), %stride_logits_p: i32 {tt.divisibility = 16 : i32} loc("stride_logits_p"(#loc)), %stride_logits_g: i32 {tt.divisibility = 16 : i32} loc("stride_logits_g"(#loc)), %stride_q_s: i32 {tt.divisibility = 16 : i32} loc("stride_q_s"(#loc)), %stride_q_nh: i32 {tt.divisibility = 16 : i32} loc("stride_q_nh"(#loc)), %stride_k_b: i32 {tt.divisibility = 16 : i32} loc("stride_k_b"(#loc)), %stride_k_nh: i32 {tt.divisibility = 16 : i32} loc("stride_k_nh"(#loc)), %stride_k_hz: i32 {tt.divisibility = 16 : i32} loc("stride_k_hz"(#loc)), %stride_k_bz: i32 loc("stride_k_bz"(#loc)), %stride_v_b: i32 {tt.divisibility = 16 : i32} loc("stride_v_b"(#loc)), %stride_v_nh: i32 {tt.divisibility = 16 : i32} loc("stride_v_nh"(#loc)), %stride_v_hz: i32 {tt.divisibility = 16 : i32} loc("stride_v_hz"(#loc)), %stride_bt_s: i32 {tt.divisibility = 16 : i32} loc("stride_bt_s"(#loc))) attributes {noinline = false} {
    %cst = arith.constant dense<8> : tensor<1x1x16x1xi32, #blocked> loc(#loc1)
    %cst_0 = arith.constant dense<8> : tensor<16x1xi32, #mma> loc(#loc1)
    %cst_1 = arith.constant dense<16> : tensor<16x1xi32, #linear> loc(#loc1)
    %cst_2 = arith.constant dense<0.000000e+00> : tensor<16x256xf32, #mma> loc(#loc1)
    %cst_3 = arith.constant dense<0.000000e+00> : tensor<16x128xf32, #mma> loc(#loc1)
    %cst_4 = arith.constant dense<128> : tensor<1x128xi32, #blocked1> loc(#loc1)
    %cst_5 = arith.constant dense<1.44269502> : tensor<16x256xf32, #mma> loc(#loc1)
    %cst_6 = arith.constant dense<0xFF800000> : tensor<16x256xf32, #mma> loc(#loc1)
    %cst_7 = arith.constant dense<8> : tensor<16xi32, #blocked2> loc(#loc1)
    %c15_i32 = arith.constant 15 : i32 loc(#loc1)
    %c8_i32 = arith.constant 8 : i32 loc(#loc1)
    %c16_i32 = arith.constant 16 : i32 loc(#loc1)
    %cst_8 = arith.constant dense<0.000000e+00> : tensor<16x128xbf16, #blocked1> loc(#loc1)
    %c256_i32 = arith.constant 256 : i32 loc(#loc1)
    %cst_9 = arith.constant dense<0> : tensor<16xi32, #ttg.slice<{dim = 1, parent = #ttg.slice<{dim = 2, parent = #blocked3}>}>> loc(#loc1)
    %cst_10 = arith.constant dense<0> : tensor<16xi32, #ttg.slice<{dim = 1, parent = #ttg.slice<{dim = 2, parent = #ttg.slice<{dim = 3, parent = #blocked}>}>}>> loc(#loc1)
    %cst_11 = arith.constant dense<8> : tensor<16x1xi32, #blocked1> loc(#loc1)
    %cst_12 = arith.constant dense<128> : tensor<1x128xi32, #mma> loc(#loc1)
    %seq_idx = tt.get_program_id x : i32 loc(#loc115)
    %kv_head_idx = tt.get_program_id y : i32 loc(#loc116)
    %seq_part_idx = tt.get_program_id z : i32 loc(#loc117)
    %seq_len = tt.addptr %seq_lens_ptr, %seq_idx : !tt.ptr<i32>, i32 loc(#loc118)
    %seq_len_13 = tt.load %seq_len : !tt.ptr<i32> loc(#loc119)
    %seq_start_idx = arith.muli %seq_part_idx, %c256_i32 : i32 loc(#loc120)
    %0 = arith.cmpi sge, %seq_start_idx, %seq_len_13 : i32 loc(#loc8)
    cf.cond_br %0, ^bb1, ^bb2 loc(#loc8)
  ^bb1:  // pred: ^bb0
    tt.return loc(#loc9)
  ^bb2:  // pred: ^bb0
    %seq_end_idx = arith.addi %seq_start_idx, %c256_i32 : i32 loc(#loc121)
    %seq_end_idx_14 = arith.minsi %seq_end_idx, %seq_len_13 : i32 loc(#loc122)
    %num_kv_blks = arith.subi %seq_end_idx_14, %seq_start_idx : i32 loc(#loc123)
    %num_kv_blks_15 = arith.addi %num_kv_blks, %c15_i32 : i32 loc(#loc185)
    %num_kv_blks_16 = arith.divsi %num_kv_blks_15, %c16_i32 : i32 loc(#loc186)
    %blk_offs = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 1, parent = #ttg.slice<{dim = 2, parent = #blocked3}>}>> loc(#loc125)
    %blk_offs_17 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 1, parent = #ttg.slice<{dim = 2, parent = #ttg.slice<{dim = 3, parent = #blocked}>}>}>> loc(#loc125)
    %blk_offs_18 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #blocked2> loc(#loc125)
    %masked_blk_ids_19 = tt.splat %num_kv_blks_16 : i32 -> tensor<16xi32, #ttg.slice<{dim = 1, parent = #ttg.slice<{dim = 2, parent = #ttg.slice<{dim = 3, parent = #blocked}>}>}>> loc(#loc126)
    %masked_blk_ids_21 = arith.cmpi slt, %blk_offs_17, %masked_blk_ids_19 : tensor<16xi32, #ttg.slice<{dim = 1, parent = #ttg.slice<{dim = 2, parent = #ttg.slice<{dim = 3, parent = #blocked}>}>}>> loc(#loc126)
    %masked_blk_ids_23 = arith.select %masked_blk_ids_21, %blk_offs_17, %cst_10 : tensor<16xi1, #ttg.slice<{dim = 1, parent = #ttg.slice<{dim = 2, parent = #ttg.slice<{dim = 3, parent = #blocked}>}>}>>, tensor<16xi32, #ttg.slice<{dim = 1, parent = #ttg.slice<{dim = 2, parent = #ttg.slice<{dim = 3, parent = #blocked}>}>}>> loc(#loc127)
    %kv_blk_start = arith.muli %seq_part_idx, %c16_i32 : i32 loc(#loc128)
    %blk_tables_start_ptr = arith.muli %seq_idx, %stride_bt_s : i32 loc(#loc129)
    %blk_tables_start_ptr_24 = tt.addptr %blk_tables_ptrs, %blk_tables_start_ptr : !tt.ptr<i32>, i32 loc(#loc130)
    %kv_blk_nums = tt.addptr %blk_tables_start_ptr_24, %kv_blk_start : !tt.ptr<i32>, i32 loc(#loc131)
    %kv_blk_nums_26 = amdgpu.buffer_load %kv_blk_nums[%masked_blk_ids_23] : tensor<16xi32, #ttg.slice<{dim = 1, parent = #ttg.slice<{dim = 2, parent = #ttg.slice<{dim = 3, parent = #blocked}>}>}>> loc(#loc132)
    %q_offs = arith.muli %seq_idx, %stride_q_s : i32 loc(#loc133)
    %q_offs_27 = arith.muli %kv_head_idx, %c8_i32 : i32 loc(#loc134)
    %q_offs_28 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc135)
    %q_offs_29 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked1}>> loc(#loc135)
    %q_offs_30 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 1, parent = #linear}>> loc(#loc135)
    %q_offs_31 = tt.expand_dims %q_offs_28 {axis = 1 : i32} : tensor<16xi32, #ttg.slice<{dim = 1, parent = #mma}>> -> tensor<16x1xi32, #mma> loc(#loc135)
    %q_offs_32 = tt.expand_dims %q_offs_29 {axis = 1 : i32} : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked1}>> -> tensor<16x1xi32, #blocked1> loc(#loc135)
    %q_offs_33 = tt.expand_dims %q_offs_30 {axis = 1 : i32} : tensor<16xi32, #ttg.slice<{dim = 1, parent = #linear}>> -> tensor<16x1xi32, #linear> loc(#loc135)
    %q_offs_34 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 0, parent = #mma}>> loc(#loc136)
    %q_offs_35 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 0, parent = #blocked1}>> loc(#loc136)
    %q_offs_36 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 0, parent = #ttg.slice<{dim = 2, parent = #blocked3}>}>> loc(#loc136)
    %q_offs_37 = tt.expand_dims %q_offs_34 {axis = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 0, parent = #mma}>> -> tensor<1x128xi32, #mma> loc(#loc136)
    %q_offs_38 = tt.expand_dims %q_offs_35 {axis = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 0, parent = #blocked1}>> -> tensor<1x128xi32, #blocked1> loc(#loc136)
    %q_mask = arith.cmpi slt, %q_offs_31, %cst_0 : tensor<16x1xi32, #mma> loc(#loc137)
    %q_mask_39 = arith.cmpi slt, %q_offs_32, %cst_11 : tensor<16x1xi32, #blocked1> loc(#loc137)
    %q_mask_40 = arith.cmpi slt, %q_offs_37, %cst_12 : tensor<1x128xi32, #mma> loc(#loc138)
    %q_mask_41 = arith.cmpi slt, %q_offs_38, %cst_4 : tensor<1x128xi32, #blocked1> loc(#loc138)
    %q_mask_42 = tt.broadcast %q_mask : tensor<16x1xi1, #mma> -> tensor<16x128xi1, #mma> loc(#loc139)
    %q_mask_43 = tt.broadcast %q_mask_39 : tensor<16x1xi1, #blocked1> -> tensor<16x128xi1, #blocked1> loc(#loc139)
    %q_mask_44 = tt.broadcast %q_mask_40 : tensor<1x128xi1, #mma> -> tensor<16x128xi1, #mma> loc(#loc139)
    %q_mask_45 = tt.broadcast %q_mask_41 : tensor<1x128xi1, #blocked1> -> tensor<16x128xi1, #blocked1> loc(#loc139)
    %q_mask_46 = arith.andi %q_mask_42, %q_mask_44 : tensor<16x128xi1, #mma> loc(#loc139)
    %q_mask_47 = arith.andi %q_mask_43, %q_mask_45 : tensor<16x128xi1, #blocked1> loc(#loc139)
    %q = arith.muli %q_offs_27, %stride_q_nh : i32 loc(#loc140)
    %q_48 = tt.splat %stride_q_nh : i32 -> tensor<16x1xi32, #blocked1> loc(#loc140)
    %q_49 = arith.muli %q_offs_32, %q_48 : tensor<16x1xi32, #blocked1> loc(#loc140)
    %q_50 = arith.addi %q_offs, %q : i32 loc(#loc140)
    %q_51 = tt.broadcast %q_49 : tensor<16x1xi32, #blocked1> -> tensor<16x128xi32, #blocked1> loc(#loc140)
    %q_52 = tt.broadcast %q_offs_38 : tensor<1x128xi32, #blocked1> -> tensor<16x128xi32, #blocked1> loc(#loc140)
    %q_53 = arith.addi %q_51, %q_52 : tensor<16x128xi32, #blocked1> loc(#loc140)
    %q_54 = tt.addptr %q_ptr, %q_50 : !tt.ptr<bf16>, i32 loc(#loc140)
    %q_55 = tt.splat %q_54 : !tt.ptr<bf16> -> tensor<16x128x!tt.ptr<bf16>, #blocked1> loc(#loc141)
    %q_56 = tt.addptr %q_55, %q_53 : tensor<16x128x!tt.ptr<bf16>, #blocked1>, tensor<16x128xi32, #blocked1> loc(#loc141)
    %q_57 = tt.load %q_56, %q_mask_47, %cst_8 : tensor<16x128x!tt.ptr<bf16>, #blocked1> loc(#loc141)
    %q_58 = arith.extf %q_57 : tensor<16x128xbf16, #blocked1> to tensor<16x128xf32, #blocked1> loc(#loc142)
    %q_59 = tt.splat %scale : f32 -> tensor<16x128xf32, #blocked1> loc(#loc142)
    %q_60 = arith.mulf %q_58, %q_59 : tensor<16x128xf32, #blocked1> loc(#loc142)
    %q_61 = arith.truncf %q_60 : tensor<16x128xf32, #blocked1> to tensor<16x128xbf16, #blocked1> loc(#loc143)
    %q_104 = ttg.local_alloc %q_61 : (tensor<16x128xbf16, #blocked1>) -> !ttg.memdesc<16x128xbf16, #shared, #smem> loc(#loc143)
    %q_105 = ttg.local_load %q_104 : !ttg.memdesc<16x128xbf16, #shared, #smem> -> tensor<16x128xbf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 8}>> loc(#loc143)
    // %q_105 = ttg.implicit_convert_layout %q_105_0 : tensor<16x128xbf16, #linear4> -> tensor<16x128xbf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 4}>> loc(#loc143)

    %k_blk_offs = arith.muli %kv_head_idx, %stride_k_nh : i32 loc(#loc144)
    %k_blk_offs_62 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #linear}>> loc(#loc145)
    %k_blk_offs_63 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #ttg.slice<{dim = 2, parent = #ttg.slice<{dim = 3, parent = #blocked}>}>}>> loc(#loc145)
    %k_blk_offs_64 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #ttg.slice<{dim = 1, parent = #blocked3}>}>> loc(#loc145)
    %k_blk_offs_65 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #ttg.slice<{dim = 1, parent = #ttg.slice<{dim = 3, parent = #blocked}>}>}>> loc(#loc145)
    %k_blk_offs_66 = tt.expand_dims %k_blk_offs_62 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #linear}>> -> tensor<1x16xi32, #linear> loc(#loc145)
    %k_blk_offs_67 = tt.make_range {end = 8 : i32, start = 0 : i32} : tensor<8xi32, #ttg.slice<{dim = 0, parent = #ttg.slice<{dim = 1, parent = #ttg.slice<{dim = 2, parent = #blocked}>}>}>> loc(#loc146)
    %blk_seq_offs = tt.splat %kv_blk_start : i32 -> tensor<16x1xi32, #linear> loc(#loc147)
    %blk_seq_offs_68 = arith.addi %blk_seq_offs, %q_offs_33 : tensor<16x1xi32, #linear> loc(#loc147)
    %blk_seq_offs_69 = arith.muli %blk_seq_offs_68, %cst_1 : tensor<16x1xi32, #linear> loc(#loc148)
    %blk_seq_offs_70 = tt.broadcast %blk_seq_offs_69 : tensor<16x1xi32, #linear> -> tensor<16x16xi32, #linear> loc(#loc149)
    %blk_seq_offs_71 = tt.broadcast %k_blk_offs_66 : tensor<1x16xi32, #linear> -> tensor<16x16xi32, #linear> loc(#loc149)
    %blk_seq_offs_72 = arith.addi %blk_seq_offs_70, %blk_seq_offs_71 : tensor<16x16xi32, #linear> loc(#loc149)
    %k_0 = tt.expand_dims %kv_blk_nums_26 {axis = 1 : i32} : tensor<16xi32, #ttg.slice<{dim = 1, parent = #ttg.slice<{dim = 2, parent = #ttg.slice<{dim = 3, parent = #blocked}>}>}>> -> tensor<16x1xi32, #ttg.slice<{dim = 2, parent = #ttg.slice<{dim = 3, parent = #blocked}>}>> loc(#loc150)
    %k_0_73 = tt.expand_dims %k_0 {axis = 2 : i32} : tensor<16x1xi32, #ttg.slice<{dim = 2, parent = #ttg.slice<{dim = 3, parent = #blocked}>}>> -> tensor<16x1x1xi32, #ttg.slice<{dim = 3, parent = #blocked}>> loc(#loc150)
    %k_0_74 = tt.expand_dims %k_0_73 {axis = 3 : i32} : tensor<16x1x1xi32, #ttg.slice<{dim = 3, parent = #blocked}>> -> tensor<16x1x1x1xi32, #blocked> loc(#loc150)
    %k_0_75 = tt.splat %stride_k_b : i32 -> tensor<16x1x1x1xi32, #blocked> loc(#loc150)
    %k_0_76 = arith.muli %k_0_74, %k_0_75 : tensor<16x1x1x1xi32, #blocked> loc(#loc150)
    %k_0_77 = tt.broadcast %k_0_76 : tensor<16x1x1x1xi32, #blocked> -> tensor<16x16x1x1xi32, #blocked> loc(#loc150)
    %k_0_78 = tt.expand_dims %k_blk_offs_63 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #ttg.slice<{dim = 2, parent = #ttg.slice<{dim = 3, parent = #blocked}>}>}>> -> tensor<1x16xi32, #ttg.slice<{dim = 2, parent = #ttg.slice<{dim = 3, parent = #blocked}>}>> loc(#loc150)
    %k_0_79 = tt.expand_dims %k_0_78 {axis = 2 : i32} : tensor<1x16xi32, #ttg.slice<{dim = 2, parent = #ttg.slice<{dim = 3, parent = #blocked}>}>> -> tensor<1x16x1xi32, #ttg.slice<{dim = 3, parent = #blocked}>> loc(#loc150)
    %k_0_80 = tt.expand_dims %k_0_79 {axis = 3 : i32} : tensor<1x16x1xi32, #ttg.slice<{dim = 3, parent = #blocked}>> -> tensor<1x16x1x1xi32, #blocked> loc(#loc150)
    %k_0_81 = tt.splat %stride_k_hz : i32 -> tensor<1x16x1x1xi32, #blocked> loc(#loc150)
    %k_0_82 = arith.muli %k_0_80, %k_0_81 : tensor<1x16x1x1xi32, #blocked> loc(#loc150)
    %k_0_83 = tt.broadcast %k_0_82 : tensor<1x16x1x1xi32, #blocked> -> tensor<16x16x1x1xi32, #blocked> loc(#loc150)
    %k_0_84 = arith.addi %k_0_77, %k_0_83 : tensor<16x16x1x1xi32, #blocked> loc(#loc150)
    %k_0_85 = tt.broadcast %k_0_84 : tensor<16x16x1x1xi32, #blocked> -> tensor<16x16x16x1xi32, #blocked> loc(#loc150)
    %k_0_86 = tt.expand_dims %k_blk_offs_65 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #ttg.slice<{dim = 1, parent = #ttg.slice<{dim = 3, parent = #blocked}>}>}>> -> tensor<1x16xi32, #ttg.slice<{dim = 1, parent = #ttg.slice<{dim = 3, parent = #blocked}>}>> loc(#loc150)
    %k_0_87 = tt.expand_dims %k_0_86 {axis = 1 : i32} : tensor<1x16xi32, #ttg.slice<{dim = 1, parent = #ttg.slice<{dim = 3, parent = #blocked}>}>> -> tensor<1x1x16xi32, #ttg.slice<{dim = 3, parent = #blocked}>> loc(#loc150)
    %k_0_88 = tt.expand_dims %k_0_87 {axis = 3 : i32} : tensor<1x1x16xi32, #ttg.slice<{dim = 3, parent = #blocked}>> -> tensor<1x1x16x1xi32, #blocked> loc(#loc150)
    %k_0_89 = arith.muli %k_0_88, %cst : tensor<1x1x16x1xi32, #blocked> loc(#loc150)
    %k_0_90 = tt.broadcast %k_0_89 : tensor<1x1x16x1xi32, #blocked> -> tensor<16x16x16x1xi32, #blocked> loc(#loc150)
    %k_0_91 = arith.addi %k_0_85, %k_0_90 : tensor<16x16x16x1xi32, #blocked> loc(#loc150)
    %k_0_92 = tt.broadcast %k_0_91 : tensor<16x16x16x1xi32, #blocked> -> tensor<16x16x16x8xi32, #blocked> loc(#loc150)
    %k_0_93 = tt.expand_dims %k_blk_offs_67 {axis = 0 : i32} : tensor<8xi32, #ttg.slice<{dim = 0, parent = #ttg.slice<{dim = 1, parent = #ttg.slice<{dim = 2, parent = #blocked}>}>}>> -> tensor<1x8xi32, #ttg.slice<{dim = 1, parent = #ttg.slice<{dim = 2, parent = #blocked}>}>> loc(#loc150)
    %k_0_94 = tt.expand_dims %k_0_93 {axis = 1 : i32} : tensor<1x8xi32, #ttg.slice<{dim = 1, parent = #ttg.slice<{dim = 2, parent = #blocked}>}>> -> tensor<1x1x8xi32, #ttg.slice<{dim = 2, parent = #blocked}>> loc(#loc150)
    %k_0_95 = tt.expand_dims %k_0_94 {axis = 2 : i32} : tensor<1x1x8xi32, #ttg.slice<{dim = 2, parent = #blocked}>> -> tensor<1x1x1x8xi32, #blocked> loc(#loc150)
    %k_0_96 = tt.broadcast %k_0_95 : tensor<1x1x1x8xi32, #blocked> -> tensor<16x16x16x8xi32, #blocked> loc(#loc150)
    %k_0_97 = arith.addi %k_0_92, %k_0_96 : tensor<16x16x16x8xi32, #blocked> loc(#loc150)
    %k_0_98 = tt.addptr %k_cache_ptr, %k_blk_offs : !tt.ptr<bf16>, i32 loc(#loc150)
    %k_0_99 = tt.splat %k_0_98 : !tt.ptr<bf16> -> tensor<16x16x16x8x!tt.ptr<bf16>, #blocked> loc(#loc151)
    %k_0_100 = tt.addptr %k_0_99, %k_0_97 : tensor<16x16x16x8x!tt.ptr<bf16>, #blocked>, tensor<16x16x16x8xi32, #blocked> loc(#loc151)
    %k_0_101 = tt.load %k_0_100 : tensor<16x16x16x8x!tt.ptr<bf16>, #blocked> loc(#loc151)
    // %k = ttg.convert_layout %k_0_101 : tensor<16x16x16x8xbf16, #blocked> -> tensor<16x16x16x8xbf16, #linear1> loc(#loc152)
    %k_102 = tt.trans %k_0_101 {order = array<i32: 1, 3, 0, 2>} : tensor<16x16x16x8xbf16, #blocked> -> tensor<16x8x16x16xbf16, #linear2> loc(#loc153)
    %k_103 = tt.reshape %k_102 : tensor<16x8x16x16xbf16, #linear2> -> tensor<128x256xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 8}>> loc(#loc152)
    // %k_103 = ttg.implicit_convert_layout %k_0_101 : tensor<16x16x16x8xbf16, #blocked> -> tensor<128x256xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 4}>> loc(#loc153)

    %qk = tt.dot %q_105, %k_103, %cst_2, inputPrecision = tf32 : tensor<16x128xbf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 8}>> * tensor<128x256xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 8}>> -> tensor<16x256xf32, #mma> loc(#loc154)
    %blk_seq_flatten_offs = tt.reshape %blk_seq_offs_72 : tensor<16x16xi32, #linear> -> tensor<256xi32, #ttg.slice<{dim = 0, parent = #mma}>> loc(#loc155)
    %qk_106 = tt.expand_dims %blk_seq_flatten_offs {axis = 0 : i32} : tensor<256xi32, #ttg.slice<{dim = 0, parent = #mma}>> -> tensor<1x256xi32, #mma> loc(#loc156)
    %qk_107 = tt.splat %seq_len_13 : i32 -> tensor<1x256xi32, #mma> loc(#loc157)
    %qk_108 = arith.cmpi slt, %qk_106, %qk_107 : tensor<1x256xi32, #mma> loc(#loc157)
    %qk_109 = tt.broadcast %q_mask : tensor<16x1xi1, #mma> -> tensor<16x256xi1, #mma> loc(#loc158)
    %qk_110 = tt.broadcast %qk_108 : tensor<1x256xi1, #mma> -> tensor<16x256xi1, #mma> loc(#loc158)
    %qk_111 = arith.andi %qk_109, %qk_110 : tensor<16x256xi1, #mma> loc(#loc158)
    %qk_112 = arith.select %qk_111, %qk, %cst_6 : tensor<16x256xi1, #mma>, tensor<16x256xf32, #mma> loc(#loc159)
    %max_logit_new = "tt.reduce"(%qk_112) <{axis = 1 : i32}> ({
    ^bb0(%max_logit_new_152: f32 loc(callsite(#loc1 at #loc160)), %max_logit_new_153: f32 loc(callsite(#loc1 at #loc160))):
      %max_logit_new_154 = arith.maxnumf %max_logit_new_152, %max_logit_new_153 : f32 loc(#loc191)
      tt.reduce.return %max_logit_new_154 : f32 loc(#loc187)
    }) : (tensor<16x256xf32, #mma>) -> tensor<16xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc187)
    %p = tt.expand_dims %max_logit_new {axis = 1 : i32} : tensor<16xf32, #ttg.slice<{dim = 1, parent = #mma}>> -> tensor<16x1xf32, #mma> loc(#loc161)
    %p_113 = tt.broadcast %p : tensor<16x1xf32, #mma> -> tensor<16x256xf32, #mma> loc(#loc162)
    %p_114 = arith.subf %qk_112, %p_113 : tensor<16x256xf32, #mma> loc(#loc162)
    %p_115 = arith.mulf %p_114, %cst_5 : tensor<16x256xf32, #mma> loc(#loc163)
    %p_116 = math.exp2 %p_115 : tensor<16x256xf32, #mma> loc(#loc164)
    %p_117 = arith.truncf %p_116 : tensor<16x256xf32, #mma> to tensor<16x256xbf16, #mma> loc(#loc165)
    %exp_sum = "tt.reduce"(%p_117) <{axis = 1 : i32}> ({
    ^bb0(%exp_sum_152: bf16 loc(callsite(#loc1 at #loc166)), %exp_sum_153: bf16 loc(callsite(#loc1 at #loc166))):
      %exp_sum_154 = arith.addf %exp_sum_152, %exp_sum_153 : bf16 loc(#loc192)
      tt.reduce.return %exp_sum_154 : bf16 loc(#loc189)
    }) : (tensor<16x256xbf16, #mma>) -> tensor<16xbf16, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc189)
    %p_143 = ttg.local_alloc %p_117 : (tensor<16x256xbf16, #mma>) -> !ttg.memdesc<16x256xbf16, #shared1, #smem> loc(#loc165)
    %p_144 = ttg.local_load %p_143 : !ttg.memdesc<16x256xbf16, #shared1, #smem> -> tensor<16x256xbf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 16}>> loc(#loc165)
    // %p_144 = ttg.implicit_convert_layout %p_144_0 : tensor<16x256xbf16, #linear5> -> tensor<16x256xbf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 8}>> loc(#loc165)

    %masked_blk_ids = tt.splat %num_kv_blks_16 : i32 -> tensor<16xi32, #ttg.slice<{dim = 1, parent = #ttg.slice<{dim = 2, parent = #blocked3}>}>> loc(#loc126)
    %masked_blk_ids_20 = arith.cmpi slt, %blk_offs, %masked_blk_ids : tensor<16xi32, #ttg.slice<{dim = 1, parent = #ttg.slice<{dim = 2, parent = #blocked3}>}>> loc(#loc126)
    %masked_blk_ids_22 = arith.select %masked_blk_ids_20, %blk_offs, %cst_9 : tensor<16xi1, #ttg.slice<{dim = 1, parent = #ttg.slice<{dim = 2, parent = #blocked3}>}>>, tensor<16xi32, #ttg.slice<{dim = 1, parent = #ttg.slice<{dim = 2, parent = #blocked3}>}>> loc(#loc127)
    %kv_blk_nums_25 = amdgpu.buffer_load %kv_blk_nums[%masked_blk_ids_22] : tensor<16xi32, #ttg.slice<{dim = 1, parent = #ttg.slice<{dim = 2, parent = #blocked3}>}>> loc(#loc132)
    %v_blk_offs = arith.muli %kv_head_idx, %stride_v_nh : i32 loc(#loc167)
    %v_0 = tt.expand_dims %kv_blk_nums_25 {axis = 1 : i32} : tensor<16xi32, #ttg.slice<{dim = 1, parent = #ttg.slice<{dim = 2, parent = #blocked3}>}>> -> tensor<16x1xi32, #ttg.slice<{dim = 2, parent = #blocked3}>> loc(#loc168)
    %v_0_118 = tt.expand_dims %v_0 {axis = 2 : i32} : tensor<16x1xi32, #ttg.slice<{dim = 2, parent = #blocked3}>> -> tensor<16x1x1xi32, #blocked3> loc(#loc168)
    %v_0_119 = tt.splat %stride_v_b : i32 -> tensor<16x1x1xi32, #blocked3> loc(#loc168)
    %v_0_120 = arith.muli %v_0_118, %v_0_119 : tensor<16x1x1xi32, #blocked3> loc(#loc168)
    %v_0_121 = tt.broadcast %v_0_120 : tensor<16x1x1xi32, #blocked3> -> tensor<16x128x1xi32, #blocked3> loc(#loc168)
    %v_0_122 = tt.expand_dims %q_offs_36 {axis = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 0, parent = #ttg.slice<{dim = 2, parent = #blocked3}>}>> -> tensor<1x128xi32, #ttg.slice<{dim = 2, parent = #blocked3}>> loc(#loc168)
    %v_0_123 = tt.expand_dims %v_0_122 {axis = 2 : i32} : tensor<1x128xi32, #ttg.slice<{dim = 2, parent = #blocked3}>> -> tensor<1x128x1xi32, #blocked3> loc(#loc168)
    %v_0_124 = tt.splat %stride_v_hz : i32 -> tensor<1x128x1xi32, #blocked3> loc(#loc168)
    %v_0_125 = arith.muli %v_0_123, %v_0_124 : tensor<1x128x1xi32, #blocked3> loc(#loc168)
    %v_0_126 = tt.broadcast %v_0_125 : tensor<1x128x1xi32, #blocked3> -> tensor<16x128x1xi32, #blocked3> loc(#loc168)
    %v_0_127 = arith.addi %v_0_121, %v_0_126 : tensor<16x128x1xi32, #blocked3> loc(#loc168)
    %v_0_128 = tt.broadcast %v_0_127 : tensor<16x128x1xi32, #blocked3> -> tensor<16x128x16xi32, #blocked3> loc(#loc168)
    %v_0_129 = tt.expand_dims %k_blk_offs_64 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #ttg.slice<{dim = 1, parent = #blocked3}>}>> -> tensor<1x16xi32, #ttg.slice<{dim = 1, parent = #blocked3}>> loc(#loc168)
    %v_0_130 = tt.expand_dims %v_0_129 {axis = 1 : i32} : tensor<1x16xi32, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<1x1x16xi32, #blocked3> loc(#loc168)
    %v_0_131 = tt.broadcast %v_0_130 : tensor<1x1x16xi32, #blocked3> -> tensor<16x128x16xi32, #blocked3> loc(#loc168)
    %v_0_132 = arith.addi %v_0_128, %v_0_131 : tensor<16x128x16xi32, #blocked3> loc(#loc168)
    %v_0_133 = tt.addptr %v_cache_ptr, %v_blk_offs : !tt.ptr<bf16>, i32 loc(#loc168)
    %v_0_134 = tt.splat %v_0_133 : !tt.ptr<bf16> -> tensor<16x128x16x!tt.ptr<bf16>, #blocked3> loc(#loc169)
    %v_0_135 = tt.addptr %v_0_134, %v_0_132 : tensor<16x128x16x!tt.ptr<bf16>, #blocked3>, tensor<16x128x16xi32, #blocked3> loc(#loc169)
    %v_0_136 = tt.load %v_0_135 : tensor<16x128x16x!tt.ptr<bf16>, #blocked3> loc(#loc169)
    // %v = ttg.convert_layout %v_0_136 : tensor<16x128x16xbf16, #blocked3> -> tensor<16x128x16xbf16, #linear3> loc(#loc170)
    %v_137 = tt.trans %v_0_136 {order = array<i32: 0, 2, 1>} : tensor<16x128x16xbf16, #blocked3> -> tensor<16x16x128xbf16, #linear6> loc(#loc171)
    %v_138 = tt.reshape %v_137 : tensor<16x16x128xbf16, #linear6> -> tensor<256x128xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 16}>> loc(#loc170)
    // %v_138 = ttg.implicit_convert_layout %v_0_136 : tensor<16x128x16xbf16, #blocked3> -> tensor<256x128xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 8}>> loc(#loc170)

    %max_logits_offs = arith.muli %seq_idx, %stride_max_logits_s : i32 loc(#loc172)
    %max_logits_offs_139 = arith.muli %kv_head_idx, %stride_max_logits_nh : i32 loc(#loc173)
    %max_logits_offs_140 = arith.addi %max_logits_offs, %max_logits_offs_139 : i32 loc(#loc174)
    %max_logits_offs_141 = arith.muli %seq_part_idx, %stride_max_logits_p : i32 loc(#loc175)
    %max_logits_offs_142 = arith.addi %max_logits_offs_140, %max_logits_offs_141 : i32 loc(#loc176)
    %m_grp_mask = arith.cmpi slt, %blk_offs_18, %cst_7 : tensor<16xi32, #blocked2> loc(#loc177)
    %1 = tt.addptr %max_logits_ptr, %max_logits_offs_142 : !tt.ptr<f32>, i32 loc(#loc73)
    %2 = ttg.convert_layout %max_logit_new : tensor<16xf32, #ttg.slice<{dim = 1, parent = #mma}>> -> tensor<16xf32, #blocked2> loc(#loc74)
    amdgpu.buffer_store %2, %1[%blk_offs_18], %m_grp_mask : tensor<16xf32, #blocked2> loc(#loc74)
    %3 = tt.addptr %exp_sums_ptr, %max_logits_offs_142 : !tt.ptr<f32>, i32 loc(#loc75)
    %4 = ttg.convert_layout %exp_sum : tensor<16xbf16, #ttg.slice<{dim = 1, parent = #mma}>> -> tensor<16xbf16, #blocked2> loc(#loc76)
    %5 = arith.extf %4 : tensor<16xbf16, #blocked2> to tensor<16xf32, #blocked2> loc(#loc76)
    amdgpu.buffer_store %5, %3[%blk_offs_18], %m_grp_mask : tensor<16xf32, #blocked2> loc(#loc76)

    %acc = tt.dot %p_144, %v_138, %cst_3, inputPrecision = tf32 : tensor<16x256xbf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 16}>> * tensor<256x128xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 16}>> -> tensor<16x128xf32, #mma> loc(#loc178)
    %acc_145 = tt.expand_dims %exp_sum {axis = 1 : i32} : tensor<16xbf16, #ttg.slice<{dim = 1, parent = #mma}>> -> tensor<16x1xbf16, #mma> loc(#loc179)
    %acc_146 = arith.extf %acc_145 : tensor<16x1xbf16, #mma> to tensor<16x1xf32, #mma> loc(#loc180)
    %acc_147 = tt.broadcast %acc_146 : tensor<16x1xf32, #mma> -> tensor<16x128xf32, #mma> loc(#loc180)
    %acc_148 = arith.divf %acc, %acc_147 : tensor<16x128xf32, #mma> loc(#loc180)
    %logits_offs = arith.muli %seq_idx, %stride_logits_s : i32 loc(#loc181)
    %logits_offs_149 = arith.muli %kv_head_idx, %stride_logits_nh : i32 loc(#loc182)
    %logits_offs_150 = arith.addi %logits_offs, %logits_offs_149 : i32 loc(#loc183)
    %logits_offs_151 = arith.muli %seq_part_idx, %stride_logits_p : i32 loc(#loc184)
    %6 = tt.splat %stride_logits_g : i32 -> tensor<16x1xi32, #mma> loc(#loc84)
    %7 = arith.muli %q_offs_31, %6 : tensor<16x1xi32, #mma> loc(#loc84)
    %8 = tt.broadcast %7 : tensor<16x1xi32, #mma> -> tensor<16x128xi32, #mma> loc(#loc84)
    %9 = tt.broadcast %q_offs_37 : tensor<1x128xi32, #mma> -> tensor<16x128xi32, #mma> loc(#loc84)
    %10 = arith.addi %8, %9 : tensor<16x128xi32, #mma> loc(#loc84)
    %11 = arith.addi %logits_offs_150, %logits_offs_151 : i32 loc(#loc84)
    %12 = tt.addptr %logits_ptr, %11 : !tt.ptr<bf16>, i32 loc(#loc84)
    %13 = arith.truncf %acc_148 : tensor<16x128xf32, #mma> to tensor<16x128xbf16, #mma> loc(#loc85)
    %14 = tt.splat %12 : !tt.ptr<bf16> -> tensor<16x128x!tt.ptr<bf16>, #mma> loc(#loc85)
    %15 = tt.addptr %14, %10 : tensor<16x128x!tt.ptr<bf16>, #mma>, tensor<16x128xi32, #mma> loc(#loc85)
    tt.store %15, %13, %q_mask_46 : tensor<16x128x!tt.ptr<bf16>, #mma> loc(#loc85)
    tt.return loc(#loc86)
  } loc(#loc)
} loc(#loc)
#loc2 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":1905:28)
#loc3 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":1906:32)
#loc4 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":1907:33)
#loc5 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":1912:37)
#loc6 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":1912:22)
#loc7 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":1913:35)
#loc8 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":1914:24)
#loc9 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":1915:8)
#loc10 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":1917:45)
#loc11 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":1917:63)
#loc12 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":1919:40)
#loc13 = loc("/root/test/triton/python/triton/language/standard.py":41:22)
#loc14 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":1919:55)
#loc15 = loc("/root/test/triton/python/triton/language/standard.py":41:28)
#loc16 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":1921:28)
#loc17 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":1939:40)
#loc18 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":1939:62)
#loc19 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":1940:34)
#loc20 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":1941:55)
#loc21 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":1941:45)
#loc22 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":1942:49)
#loc23 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":1942:26)
#loc24 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":1946:18)
#loc25 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":1947:25)
#loc26 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":1947:51)
#loc27 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":1948:23)
#loc28 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":1950:36)
#loc29 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":1950:77)
#loc30 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":1950:53)
#loc31 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":1951:24)
#loc32 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":1951:16)
#loc33 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":1952:13)
#loc34 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":1952:23)
#loc35 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":1957:24)
#loc36 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":1958:27)
#loc37 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":1960:35)
#loc38 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":1963:36)
#loc39 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":1963:56)
#loc40 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":1964:22)
#loc41 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":1973:32)
#loc42 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":1973:18)
#loc43 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":1978:22)
#loc44 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":1977:22)
#loc45 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":1981:19)
#loc46 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":1982:52)
#loc47 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":1988:69)
#loc48 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":1988:80)
#loc49 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":1988:48)
#loc50 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":1990:8)
#loc51 = loc("/root/test/triton/python/triton/language/standard.py":189:40)
#loc53 = loc("/root/test/triton/python/triton/language/standard.py":168:27)
#loc54 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":1995:41)
#loc55 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":1995:27)
#loc56 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":1995:53)
#loc57 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":1995:21)
#loc58 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":1996:13)
#loc59 = loc("/root/test/triton/python/triton/language/standard.py":291:36)
#loc61 = loc("/root/test/triton/python/triton/language/standard.py":261:15)
#loc62 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":2002:24)
#loc63 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":2013:32)
#loc64 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":2013:18)
#loc65 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":2018:22)
#loc66 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":2017:22)
#loc67 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":2021:18)
#loc68 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":2022:24)
#loc69 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":2022:10)
#loc70 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":2023:25)
#loc71 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":2023:10)
#loc72 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":2026:30)
#loc73 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":2027:30)
#loc74 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":2027:47)
#loc75 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":2028:28)
#loc76 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":2028:45)
#loc77 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":2031:20)
#loc78 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":2032:24)
#loc79 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":2032:16)
#loc80 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":2035:28)
#loc81 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":2036:33)
#loc82 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":2036:19)
#loc83 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":2038:23)
#loc84 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":2042:26)
#loc85 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":2042:39)
#loc86 = loc("/root/triton/python/triton_kernels/tests/test_pa_details/pa_decode.py":2042:4)
#loc115 = loc("seq_idx"(#loc2))
#loc116 = loc("kv_head_idx"(#loc3))
#loc117 = loc("seq_part_idx"(#loc4))
#loc118 = loc("seq_len"(#loc5))
#loc119 = loc("seq_len"(#loc6))
#loc120 = loc("seq_start_idx"(#loc7))
#loc121 = loc("seq_end_idx"(#loc10))
#loc122 = loc("seq_end_idx"(#loc11))
#loc123 = loc("num_kv_blks"(#loc12))
#loc124 = loc("num_kv_blks"(#loc14))
#loc125 = loc("blk_offs"(#loc16))
#loc126 = loc("masked_blk_ids"(#loc17))
#loc127 = loc("masked_blk_ids"(#loc18))
#loc128 = loc("kv_blk_start"(#loc19))
#loc129 = loc("blk_tables_start_ptr"(#loc20))
#loc130 = loc("blk_tables_start_ptr"(#loc21))
#loc131 = loc("kv_blk_nums"(#loc22))
#loc132 = loc("kv_blk_nums"(#loc23))
#loc133 = loc("q_offs"(#loc24))
#loc134 = loc("q_offs"(#loc25))
#loc135 = loc("q_offs"(#loc26))
#loc136 = loc("q_offs"(#loc27))
#loc137 = loc("q_mask"(#loc28))
#loc138 = loc("q_mask"(#loc29))
#loc139 = loc("q_mask"(#loc30))
#loc140 = loc("q"(#loc31))
#loc141 = loc("q"(#loc32))
#loc142 = loc("q"(#loc33))
#loc143 = loc("q"(#loc34))
#loc144 = loc("k_blk_offs"(#loc35))
#loc145 = loc("k_blk_offs"(#loc36))
#loc146 = loc("k_blk_offs"(#loc37))
#loc147 = loc("blk_seq_offs"(#loc38))
#loc148 = loc("blk_seq_offs"(#loc39))
#loc149 = loc("blk_seq_offs"(#loc40))
#loc150 = loc("k_0"(#loc41))
#loc151 = loc("k_0"(#loc42))
#loc152 = loc("k"(#loc43))
#loc153 = loc("k"(#loc44))
#loc154 = loc("qk"(#loc45))
#loc155 = loc("blk_seq_flatten_offs"(#loc46))
#loc156 = loc("qk"(#loc47))
#loc157 = loc("qk"(#loc48))
#loc158 = loc("qk"(#loc49))
#loc159 = loc("qk"(#loc50))
#loc161 = loc("p"(#loc54))
#loc162 = loc("p"(#loc55))
#loc163 = loc("p"(#loc56))
#loc164 = loc("p"(#loc57))
#loc165 = loc("p"(#loc58))
#loc167 = loc("v_blk_offs"(#loc62))
#loc168 = loc("v_0"(#loc63))
#loc169 = loc("v_0"(#loc64))
#loc170 = loc("v"(#loc65))
#loc171 = loc("v"(#loc66))
#loc172 = loc("max_logits_offs"(#loc67))
#loc173 = loc("max_logits_offs"(#loc68))
#loc174 = loc("max_logits_offs"(#loc69))
#loc175 = loc("max_logits_offs"(#loc70))
#loc176 = loc("max_logits_offs"(#loc71))
#loc177 = loc("m_grp_mask"(#loc72))
#loc178 = loc("acc"(#loc77))
#loc179 = loc("acc"(#loc78))
#loc180 = loc("acc"(#loc79))
#loc181 = loc("logits_offs"(#loc80))
#loc182 = loc("logits_offs"(#loc81))
#loc183 = loc("logits_offs"(#loc82))
#loc184 = loc("logits_offs"(#loc83))
#loc185 = loc(callsite(#loc13 at #loc124))
#loc186 = loc(callsite(#loc15 at #loc124))
#loc187 = loc(callsite(#loc51 at #loc160))
#loc189 = loc(callsite(#loc59 at #loc166))
#loc191 = loc(callsite(#loc53 at #loc187))
#loc192 = loc(callsite(#loc61 at #loc189))
