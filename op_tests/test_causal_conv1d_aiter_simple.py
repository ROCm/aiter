#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç®€å•æµ‹è¯• AIter Causal Conv1D
"""

import torch
import torch.nn.functional as F
import aiter
import time

def torch_causal_conv1d_ref(x, weight, bias, use_silu):
    """
    PyTorch CPU å‚è€ƒå®ç° - ä½¿ç”¨ F.conv1d
    x: [batch, dim, seqlen]
    weight: [dim, width]
    bias: [dim] or None
    """
    batch, dim, seqlen = x.shape
    width = weight.shape[1]
    
    # Causal padding: éœ€è¦ width-1 ä¸ªå·¦ä¾§å¡«å……
    padding_left = width - 1
    x_padded = F.pad(x, (padding_left, 0), 'constant', 0)
    
    # è°ƒæ•´ weight å½¢çŠ¶ç”¨äº depthwise conv1d
    # F.conv1d éœ€è¦ weight: [out_channels, in_channels/groups, kernel_size]
    # å¯¹äº depthwise: [dim, 1, width]
    weight_reshaped = weight.unsqueeze(1)  # [dim, 1, width]
    
    # Depthwise convolution
    out = F.conv1d(x_padded, weight_reshaped, bias=bias, groups=dim)
    
    if use_silu:
        out = out / (1 + torch.exp(-out))
    
    return out

print("=" * 80)
print("AIter Causal Conv1D ç®€å•æµ‹è¯•")
print("=" * 80)

# æµ‹è¯•é…ç½®
batch = 2
dim = 256
seqlen = 1024
width = 4
dtype = torch.float16

print(f"\nğŸ“Š æµ‹è¯•é…ç½®:")
print(f"   Batch: {batch}")
print(f"   Dim: {dim}")
print(f"   Seqlen: {seqlen}")
print(f"   Width: {width}")
print(f"   Dtype: {dtype}")

# åˆ›å»ºè¾“å…¥å¼ é‡
print("\nğŸ”§ åˆ›å»ºè¾“å…¥å¼ é‡...")
x = torch.randn(batch, dim, seqlen, dtype=dtype, device="cuda")
weight = torch.randn(dim, width, dtype=dtype, device="cuda")
bias = torch.randn(dim, dtype=dtype, device="cuda")
out = torch.empty_like(x)

print(f"   x shape: {x.shape}, dtype: {x.dtype}")
print(f"   weight shape: {weight.shape}, dtype: {weight.dtype}")
print(f"   bias shape: {bias.shape}, dtype: {bias.dtype}")

# æµ‹è¯• 1: åŸºç¡€è°ƒç”¨ï¼ˆæ— æ¿€æ´»ï¼‰+ å‡†ç¡®ç‡éªŒè¯
print("\n" + "=" * 80)
print("æµ‹è¯• 1: åŸºç¡€ Causal Conv1D (æ— æ¿€æ´») + å‡†ç¡®ç‡éªŒè¯")
print("=" * 80)
try:
    # è¿è¡Œ AIter å®ç°
    aiter.causal_conv1d_fwd(out, x, weight, bias, use_silu=False)
    
    # è®¡ç®— CPU å‚è€ƒç»“æœ
    print("   è®¡ç®— CPU å‚è€ƒç»“æœ...")
    x_cpu = x.cpu().float()
    weight_cpu = weight.cpu().float()
    bias_cpu = bias.cpu().float()
    ref_cpu = torch_causal_conv1d_ref(x_cpu, weight_cpu, bias_cpu, use_silu=False)
    ref_gpu = ref_cpu.to(dtype).cuda()
    
    # è®¡ç®—è¯¯å·®
    max_error = (out - ref_gpu).abs().max().item()
    mean_error = (out - ref_gpu).abs().mean().item()
    
    print("âœ… è°ƒç”¨æˆåŠŸï¼")
    print(f"   out shape: {out.shape}")
    print(f"   out min: {out.min().item():.4f}, max: {out.max().item():.4f}, mean: {out.mean().item():.4f}")
    print(f"   ğŸ“Š å‡†ç¡®ç‡:")
    print(f"      æœ€å¤§è¯¯å·®: {max_error:.2e}")
    print(f"      å¹³å‡è¯¯å·®: {mean_error:.2e}")
    
    if max_error < 1e-2:  # fp16 ç²¾åº¦é˜ˆå€¼
        print(f"      âœ… å‡†ç¡®ç‡éªŒè¯é€šè¿‡ï¼")
    else:
        print(f"      âš ï¸  è¯¯å·®è¾ƒå¤§ï¼Œå¯èƒ½æœ‰é—®é¢˜")
        
except Exception as e:
    print(f"âŒ è°ƒç”¨å¤±è´¥: {e}")
    import traceback
    traceback.print_exc()
    exit(1)

# æµ‹è¯• 2: å¸¦ SiLU æ¿€æ´» + å‡†ç¡®ç‡éªŒè¯
print("\n" + "=" * 80)
print("æµ‹è¯• 2: Causal Conv1D + SiLU æ¿€æ´» + å‡†ç¡®ç‡éªŒè¯")
print("=" * 80)
out_silu = torch.empty_like(x)
try:
    # è¿è¡Œ AIter å®ç°
    aiter.causal_conv1d_fwd(out_silu, x, weight, bias, use_silu=True)
    
    # è®¡ç®— CPU å‚è€ƒç»“æœ
    print("   è®¡ç®— CPU å‚è€ƒç»“æœ...")
    ref_silu_cpu = torch_causal_conv1d_ref(x_cpu, weight_cpu, bias_cpu, use_silu=True)
    ref_silu_gpu = ref_silu_cpu.to(dtype).cuda()
    
    # è®¡ç®—è¯¯å·®
    max_error_silu = (out_silu - ref_silu_gpu).abs().max().item()
    mean_error_silu = (out_silu - ref_silu_gpu).abs().mean().item()
    
    print("âœ… SiLU æµ‹è¯•æˆåŠŸï¼")
    print(f"   out_silu min: {out_silu.min().item():.4f}, max: {out_silu.max().item():.4f}")
    print(f"   ğŸ“Š å‡†ç¡®ç‡:")
    print(f"      æœ€å¤§è¯¯å·®: {max_error_silu:.2e}")
    print(f"      å¹³å‡è¯¯å·®: {mean_error_silu:.2e}")
    
    # éªŒè¯ SiLU çš„æ•ˆæœï¼ˆè¾“å‡ºåº”è¯¥ä¸åŒï¼‰
    diff = (out - out_silu).abs().max().item()
    print(f"   ä¸æ— æ¿€æ´»ç‰ˆæœ¬çš„æœ€å¤§å·®å¼‚: {diff:.4f}")
    if diff > 0.01:
        print("   âœ… SiLU æ¿€æ´»ç”Ÿæ•ˆ")
    
    if max_error_silu < 1e-2:
        print(f"   âœ… SiLU å‡†ç¡®ç‡éªŒè¯é€šè¿‡ï¼")
    else:
        print(f"   âš ï¸  SiLU è¯¯å·®è¾ƒå¤§")
        
except Exception as e:
    print(f"âŒ SiLU æµ‹è¯•å¤±è´¥: {e}")

# æµ‹è¯• 3: æ—  bias + å‡†ç¡®ç‡éªŒè¯
print("\n" + "=" * 80)
print("æµ‹è¯• 3: Causal Conv1D (æ—  bias) + å‡†ç¡®ç‡éªŒè¯")
print("=" * 80)
bias_empty = torch.empty(0, dtype=dtype, device="cuda")
out_no_bias = torch.empty_like(x)
try:
    # è¿è¡Œ AIter å®ç°
    aiter.causal_conv1d_fwd(out_no_bias, x, weight, bias_empty, use_silu=False)
    
    # è®¡ç®— CPU å‚è€ƒç»“æœ
    print("   è®¡ç®— CPU å‚è€ƒç»“æœ...")
    ref_no_bias_cpu = torch_causal_conv1d_ref(x_cpu, weight_cpu, None, use_silu=False)
    ref_no_bias_gpu = ref_no_bias_cpu.to(dtype).cuda()
    
    # è®¡ç®—è¯¯å·®
    max_error_no_bias = (out_no_bias - ref_no_bias_gpu).abs().max().item()
    mean_error_no_bias = (out_no_bias - ref_no_bias_gpu).abs().mean().item()
    
    print("âœ… æ—  bias æµ‹è¯•æˆåŠŸï¼")
    print(f"   out_no_bias mean: {out_no_bias.mean().item():.4f}")
    print(f"   ğŸ“Š å‡†ç¡®ç‡:")
    print(f"      æœ€å¤§è¯¯å·®: {max_error_no_bias:.2e}")
    print(f"      å¹³å‡è¯¯å·®: {mean_error_no_bias:.2e}")
    
    if max_error_no_bias < 1e-2:
        print(f"      âœ… æ—  bias å‡†ç¡®ç‡éªŒè¯é€šè¿‡ï¼")
    else:
        print(f"      âš ï¸  è¯¯å·®è¾ƒå¤§")
        
except Exception as e:
    print(f"âŒ æ—  bias æµ‹è¯•å¤±è´¥: {e}")

# æµ‹è¯• 4: ä¸åŒæ•°æ®ç±»å‹ + å‡†ç¡®ç‡éªŒè¯
print("\n" + "=" * 80)
print("æµ‹è¯• 4: ä¸åŒæ•°æ®ç±»å‹ + å‡†ç¡®ç‡éªŒè¯")
print("=" * 80)

for test_dtype in [torch.float16, torch.bfloat16, torch.float32]:
    dtype_name = str(test_dtype).split('.')[-1]
    try:
        x_test = torch.randn(2, 128, 512, dtype=test_dtype, device="cuda")
        weight_test = torch.randn(128, 4, dtype=test_dtype, device="cuda")
        bias_test = torch.randn(128, dtype=test_dtype, device="cuda")
        out_test = torch.empty_like(x_test)
        
        # è¿è¡Œ AIter å®ç°
        aiter.causal_conv1d_fwd(out_test, x_test, weight_test, bias_test, use_silu=False)
        
        # è®¡ç®— CPU å‚è€ƒç»“æœ
        x_test_cpu = x_test.cpu().float()
        weight_test_cpu = weight_test.cpu().float()
        bias_test_cpu = bias_test.cpu().float()
        ref_test_cpu = torch_causal_conv1d_ref(x_test_cpu, weight_test_cpu, bias_test_cpu, use_silu=False)
        ref_test_gpu = ref_test_cpu.to(test_dtype).cuda()
        
        # è®¡ç®—è¯¯å·®
        max_error_test = (out_test - ref_test_gpu).abs().max().item()
        
        # æ ¹æ®æ•°æ®ç±»å‹è®¾ç½®ä¸åŒçš„é˜ˆå€¼
        threshold = 1e-2 if test_dtype in [torch.float16, torch.bfloat16] else 1e-4
        
        if max_error_test < threshold:
            print(f"   âœ… {dtype_name}: æˆåŠŸ (æœ€å¤§è¯¯å·®: {max_error_test:.2e})")
        else:
            print(f"   âš ï¸  {dtype_name}: æˆåŠŸä½†è¯¯å·®è¾ƒå¤§ (æœ€å¤§è¯¯å·®: {max_error_test:.2e})")
            
    except Exception as e:
        print(f"   âŒ {dtype_name}: å¤±è´¥ - {e}")

# æµ‹è¯• 5: æ€§èƒ½æµ‹è¯•
print("\n" + "=" * 80)
print("æµ‹è¯• 5: æ€§èƒ½æµ‹è¯•")
print("=" * 80)

# Warmup
print("   é¢„çƒ­ä¸­...")
for _ in range(10):
    aiter.causal_conv1d_fwd(out, x, weight, bias, use_silu=True)

# Benchmark
print("   æ€§èƒ½æµ‹è¯•ä¸­...")
torch.cuda.synchronize()
start = time.time()
num_iters = 100
for _ in range(num_iters):
    aiter.causal_conv1d_fwd(out, x, weight, bias, use_silu=True)
torch.cuda.synchronize()
elapsed = time.time() - start

avg_time_ms = elapsed * 1000 / num_iters
total_elements = batch * dim * seqlen
throughput = total_elements * num_iters / elapsed / 1e9

# è®¡ç®—å¸¦å®½
bytes_read = x.nbytes + weight.nbytes + bias.nbytes
bytes_write = out.nbytes
total_bytes = bytes_read + bytes_write
bandwidth_gb_s = total_bytes * num_iters / elapsed / 1e9

print(f"   âœ… å¹³å‡æ—¶é—´: {avg_time_ms:.3f} ms")
print(f"   âœ… ååé‡: {throughput:.2f} G elements/s")
print(f"   âœ… å¸¦å®½: {bandwidth_gb_s:.2f} GB/s")

# æµ‹è¯• 6: éªŒè¯å› æœæ€§
print("\n" + "=" * 80)
print("æµ‹è¯• 6: éªŒè¯å› æœæ€§ï¼ˆè¾“å‡ºä¸ä¾èµ–æœªæ¥è¾“å…¥ï¼‰")
print("=" * 80)

# åˆ›å»ºä¸¤ä¸ªè¾“å…¥ï¼Œåªåœ¨æœªæ¥ä½ç½®ä¸åŒ
x1 = torch.randn(1, 4, 10, dtype=torch.float32, device="cuda")
x2 = x1.clone()
x2[:, :, -1] = 999.0  # ä¿®æ”¹æœ€åä¸€ä¸ªä½ç½®ï¼ˆæœªæ¥ï¼‰

out1 = torch.empty_like(x1)
out2 = torch.empty_like(x2)

weight_test = torch.randn(4, 4, dtype=torch.float32, device="cuda")
bias_test = torch.randn(4, dtype=torch.float32, device="cuda")

aiter.causal_conv1d_fwd(out1, x1, weight_test, bias_test, use_silu=False)
aiter.causal_conv1d_fwd(out2, x2, weight_test, bias_test, use_silu=False)

# æ£€æŸ¥å‰ 9 ä¸ªä½ç½®æ˜¯å¦ç›¸åŒï¼ˆä¸åº”è¯¥å—æœªæ¥å½±å“ï¼‰
diff_past = (out1[:, :, :-1] - out2[:, :, :-1]).abs().max().item()
print(f"   å‰ 9 ä¸ªä½ç½®çš„æœ€å¤§å·®å¼‚: {diff_past:.6f}")
if diff_past < 1e-5:
    print("   âœ… å› æœæ€§éªŒè¯é€šè¿‡ï¼è¾“å‡ºä¸ä¾èµ–æœªæ¥è¾“å…¥")
else:
    print("   âš ï¸  å› æœæ€§å¯èƒ½æœ‰é—®é¢˜")

# æœ€åä¸€ä¸ªä½ç½®åº”è¯¥ä¸åŒï¼ˆå—å½“å‰è¾“å…¥å½±å“ï¼‰
diff_current = (out1[:, :, -1] - out2[:, :, -1]).abs().max().item()
print(f"   æœ€åä½ç½®çš„æœ€å¤§å·®å¼‚: {diff_current:.6f}")
if diff_current > 0.1:
    print("   âœ… å½“å‰ä½ç½®æ­£ç¡®å“åº”è¾“å…¥å˜åŒ–")

# æ€»ç»“
print("\n" + "=" * 80)
print("âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
print("=" * 80)
print("\nğŸ“– æ›´å¤šæµ‹è¯•:")
print("   python op_tests/test_causal_conv1d.py")
print("\nğŸ“š æŸ¥çœ‹æ–‡æ¡£:")
print("   cat csrc/kernels/CAUSAL_CONV1D_INTEGRATION.md")

