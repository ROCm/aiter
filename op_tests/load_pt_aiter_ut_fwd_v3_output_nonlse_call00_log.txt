[aiter INFO] 2025-11-20 06:36:55.905 - MainProcess:11147 - /home/memin/code/aiter/aiter/jit/core.py:133 - get_config_file
merge tuned file under model_configs/ and configs/ /home/memin/code/aiter/aiter/configs/a8w8_bpreshuffle_tuned_gemm.csv:/home/memin/code/aiter/aiter/configs/model_configs/a8w8_bpreshuffle_tuned_gemm_dsv3.csv
[aiter INFO] 2025-11-20 06:36:55.916 - MainProcess:11147 - /home/memin/code/aiter/aiter/jit/core.py:133 - get_config_file
merge tuned file under model_configs/ and configs/ /home/memin/code/aiter/aiter/configs/a8w8_blockscale_tuned_gemm.csv:/home/memin/code/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_ds_v3.csv:/home/memin/code/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_qwen3_235b.csv
[aiter INFO] 2025-11-20 06:36:55.925 - MainProcess:11147 - /home/memin/code/aiter/aiter/jit/core.py:133 - get_config_file
merge tuned file under model_configs/ and configs/ /home/memin/code/aiter/aiter/configs/tuned_fmoe.csv:/home/memin/code/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[aiter INFO] 2025-11-20 06:36:55.938 - MainProcess:11147 - /home/memin/code/aiter/aiter/jit/core.py:133 - get_config_file
merge tuned file under model_configs/ and configs/ /home/memin/code/aiter/aiter/configs/a8w8_blockscale_bpreshuffle_tuned_gemm.csv:/home/memin/code/aiter/aiter/configs/model_configs/a8w8_blockscale_bpreshuffle_tuned_gemm_dsv3.csv
[aiter INFO] 2025-11-20 06:36:56.288 - MainProcess:11147 - /home/memin/code/aiter/aiter/jit/core.py:410 - get_module_custom_op
import [module_aiter_enum] under /home/memin/code/aiter/aiter/jit/module_aiter_enum.so
[aiter INFO] 2025-11-20 06:36:56.288 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling _ActivationType(dummy                        = 0)
[aiter INFO] 2025-11-20 06:36:56.288 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling _QuantType(dummy                        = 0)
[aiter INFO] 2025-11-20 06:36:56.383 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling flash_attn_varlen_func_benchmark(batch_size                   = 1,
                                         nheads                       = 32,
                                         seqlen_q                     = 2048,
                                         seqlen_k                     = 2048,
                                         d                            = 128,
                                         d_v                          = 128,
                                         min_seqlen_q                 = 1,
                                         dropout_p                    = 0.0,
                                         causal                       = True,
                                         local                        = False,
                                         bias_type                    = no,
                                         deterministic                = False,
                                         mha_type                     = gqa,
                                         dtype                        = torch.bfloat16,
                                         input_layout                 = BSHD)
[aiter INFO] 2025-11-20 06:36:57.139 - MainProcess:11147 - /home/memin/code/aiter/aiter/jit/core.py:410 - get_module_custom_op
import [module_fmha_v3_varlen_fwd] under /home/memin/code/aiter/aiter/jit/module_fmha_v3_varlen_fwd.so
[aiter WARNING] 2025-11-20 06:36:57.141 - MainProcess:11147 - /home/memin/code/aiter/aiter/jit/core.py:860 - check_args
type hints mismatch, override to --> fmha_v3_varlen_fwd(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, cu_seqlens_k: Optional[torch.Tensor], max_seqlen_q: int, max_seqlen_k: int, min_seqlen_q: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, how_v3_bf16_cvt: int, out: Optional[torch.Tensor] = None, block_table: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> List[torch.Tensor]
[aiter INFO] 2025-11-20 06:36:57.141 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.142 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.143 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.143 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.144 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.144 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.144 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.145 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.145 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.145 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.146 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.146 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.146 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.147 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.147 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.147 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.148 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.148 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.148 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.149 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.149 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.149 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.150 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.150 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.151 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.151 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.151 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.152 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.152 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.152 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.153 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.153 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.153 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.154 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.154 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.154 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.155 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.155 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.155 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.156 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.156 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.156 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.157 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.157 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.157 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.158 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.158 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.158 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.159 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.159 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.159 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.160 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.160 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.160 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.161 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.161 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.161 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.162 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.162 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.162 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.163 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.163 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.163 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.164 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.164 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.164 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.165 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.165 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.166 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.166 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.166 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.167 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.167 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.167 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.168 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.168 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.168 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.169 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.169 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.169 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.170 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.170 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.170 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.171 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.171 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.171 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.172 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.172 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.172 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.173 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.173 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.173 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.174 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.174 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.174 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.175 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.175 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.175 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.176 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.176 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.176 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.177 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.177 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.177 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:58 - wrapper
avg: 319.6591489975996 us/iter from cuda.Event
[aiter INFO] 2025-11-20 06:36:57.399 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.399 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.400 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.400 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.400 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.400 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.401 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.401 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.401 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.401 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.401 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.402 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.402 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.402 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.402 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.402 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.403 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.403 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.403 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.403 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.403 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.404 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.404 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.404 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.404 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.405 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.405 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.405 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.405 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.405 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.406 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.406 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.406 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.406 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.406 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.407 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.407 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.407 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.407 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.407 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.408 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.408 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.408 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.408 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.409 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.409 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.409 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.409 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.410 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.410 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.410 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.410 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.410 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.411 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.411 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.411 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.411 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.411 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.412 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.412 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.412 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.412 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.412 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.413 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.413 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.413 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.413 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.413 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.414 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.414 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.414 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.414 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.414 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.415 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.415 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.415 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.415 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.415 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.416 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.416 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.416 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.416 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.416 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.417 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.417 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.417 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.417 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.418 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.418 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.418 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.418 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.418 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.419 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.419 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.419 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.419 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.419 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.420 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.420 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.420 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:36:57.420 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b39a00000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b41e00000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f4b42200000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f0b2d000000,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = 0,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f0b0f800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[W1120 06:36:57.630098912 collection.cpp:1116] Warning: ROCTracer produced duplicate flow start: 1 (function operator())
[aiter INFO] 2025-11-20 06:36:57.478 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:282 - post_process_data
abnormal data indices: {6}
[aiter INFO] 2025-11-20 06:36:57.478 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:284 - post_process_data
abnormal data: 114.06200000000001
[aiter INFO] 2025-11-20 06:36:57.507 - MainProcess:11147 - /home/memin/code/aiter/aiter/test_common.py:363 - get_trace_perf
                                                       name   cnt  host_time_sum  device_time_sum device_type device_index
0              aiter::fmha_fwd_hd128_bf16_causal_rtna_group 100.0            0.0         13,452.9        CUDA            2
1                                 aiter::fmha_v3_varlen_fwd 100.0       13,452.9              0.0         CPU        11147
[avg us/iter]                                           NaN   NaN          134.5            134.5         NaN          NaN
[aiter INFO] 2025-11-20 06:37:04.453 - MainProcess:11147 - /home/memin/code/aiter/op_tests/test_mha_varlen_loadpt.py:1126 - <module>
mha_varlen summary:
   batch_size  nheads  seqlen_q  seqlen_k    d  d_v  min_seqlen_q  dropout_p  causal  local bias_type  deterministic mha_type           dtype input_layout  fwd_us  fwd_tflops  fwd_gb_per_sec
0           1      32      2048      2048  128  128             1        0.0    True  False        no          False      gqa  torch.bfloat16         BSHD   134.5       510.8           498.8
Output max diff: 0.0078125
Output Pytorch max diff: 0.025390625
