[aiter INFO] 2025-11-20 07:11:32.259 - MainProcess:11766 - /home/memin/code/aiter/aiter/jit/core.py:133 - get_config_file
merge tuned file under model_configs/ and configs/ /home/memin/code/aiter/aiter/configs/a8w8_bpreshuffle_tuned_gemm.csv:/home/memin/code/aiter/aiter/configs/model_configs/a8w8_bpreshuffle_tuned_gemm_dsv3.csv
[aiter INFO] 2025-11-20 07:11:32.270 - MainProcess:11766 - /home/memin/code/aiter/aiter/jit/core.py:133 - get_config_file
merge tuned file under model_configs/ and configs/ /home/memin/code/aiter/aiter/configs/a8w8_blockscale_tuned_gemm.csv:/home/memin/code/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_ds_v3.csv:/home/memin/code/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_qwen3_235b.csv
[aiter INFO] 2025-11-20 07:11:32.278 - MainProcess:11766 - /home/memin/code/aiter/aiter/jit/core.py:133 - get_config_file
merge tuned file under model_configs/ and configs/ /home/memin/code/aiter/aiter/configs/tuned_fmoe.csv:/home/memin/code/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[aiter INFO] 2025-11-20 07:11:32.291 - MainProcess:11766 - /home/memin/code/aiter/aiter/jit/core.py:133 - get_config_file
merge tuned file under model_configs/ and configs/ /home/memin/code/aiter/aiter/configs/a8w8_blockscale_bpreshuffle_tuned_gemm.csv:/home/memin/code/aiter/aiter/configs/model_configs/a8w8_blockscale_bpreshuffle_tuned_gemm_dsv3.csv
[aiter INFO] 2025-11-20 07:11:32.640 - MainProcess:11766 - /home/memin/code/aiter/aiter/jit/core.py:410 - get_module_custom_op
import [module_aiter_enum] under /home/memin/code/aiter/aiter/jit/module_aiter_enum.so
[aiter INFO] 2025-11-20 07:11:32.640 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling _ActivationType(dummy                        = 0)
[aiter INFO] 2025-11-20 07:11:32.640 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling _QuantType(dummy                        = 0)
[aiter INFO] 2025-11-20 07:11:32.732 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling flash_attn_varlen_func_benchmark(batch_size                   = 1,
                                         nheads                       = 32,
                                         seqlen_q                     = 2048,
                                         seqlen_k                     = 2048,
                                         d                            = 128,
                                         d_v                          = 128,
                                         min_seqlen_q                 = 1,
                                         dropout_p                    = 0.0,
                                         causal                       = True,
                                         local                        = False,
                                         bias_type                    = no,
                                         deterministic                = False,
                                         mha_type                     = gqa,
                                         dtype                        = torch.bfloat16,
                                         input_layout                 = BSHD)
[aiter INFO] 2025-11-20 07:11:33.195 - MainProcess:11766 - /home/memin/code/aiter/aiter/jit/core.py:410 - get_module_custom_op
import [module_fmha_v3_varlen_fwd] under /home/memin/code/aiter/aiter/jit/module_fmha_v3_varlen_fwd.so
[aiter WARNING] 2025-11-20 07:11:33.196 - MainProcess:11766 - /home/memin/code/aiter/aiter/jit/core.py:860 - check_args
type hints mismatch, override to --> fmha_v3_varlen_fwd(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, cu_seqlens_k: Optional[torch.Tensor], max_seqlen_q: int, max_seqlen_k: int, min_seqlen_q: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, how_v3_bf16_cvt: int, out: Optional[torch.Tensor] = None, block_table: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> List[torch.Tensor]
[aiter INFO] 2025-11-20 07:11:33.197 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.198 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.198 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.199 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.199 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.200 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.201 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.201 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.202 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.202 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.203 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.203 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.204 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.204 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.205 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.206 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.206 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.207 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.207 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.208 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.208 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.209 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.209 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.210 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.210 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.211 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.211 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.212 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.212 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.213 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.214 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.214 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.215 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.215 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.216 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.216 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.217 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.217 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.218 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.218 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.219 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.219 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.220 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.220 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.221 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.221 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.222 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.223 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.223 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.224 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.224 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.225 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.225 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.226 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.226 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.227 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.227 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.228 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.228 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.229 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.229 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.230 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.230 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.231 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.231 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.232 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.232 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.233 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.234 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.234 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.235 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.235 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.236 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.236 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.237 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.237 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.238 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.238 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.239 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.239 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.240 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.240 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.241 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.241 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.242 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.242 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.243 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.243 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.244 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.245 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.245 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.246 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.246 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.247 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.247 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.248 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.248 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.249 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.249 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.250 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.250 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.251 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.251 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.252 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:58 - wrapper
avg: 513.6625161855528 us/iter from cuda.Event
[aiter INFO] 2025-11-20 07:11:33.370 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.371 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.371 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.371 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.371 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.371 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.371 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.372 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.372 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.372 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.372 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.372 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.372 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.372 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.372 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.372 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.373 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.373 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.373 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.373 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.373 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.373 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.373 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.373 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.373 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.373 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.374 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.374 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.374 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.374 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.374 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.374 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.374 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.374 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.374 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.375 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.375 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.375 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.375 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.375 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.375 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.375 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.375 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.376 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.376 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.376 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.376 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.376 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.376 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.376 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.376 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.377 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.377 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.377 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.377 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.377 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.377 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.377 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.377 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.377 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.378 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.378 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.378 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.378 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.378 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.378 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.378 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.378 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.378 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.378 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.379 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.379 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.379 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.379 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.379 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.379 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.379 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.379 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.379 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.380 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.380 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.380 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.380 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.380 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.380 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.380 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.380 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.380 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.381 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.381 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.381 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.381 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.381 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.381 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.381 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.381 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.381 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.382 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.382 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.382 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 07:11:33.382 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7fbb4d800000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7fd350a00000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f9320e00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600400,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7fd350600000,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f931e600000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[W1120 07:11:33.621057981 collection.cpp:1116] Warning: ROCTracer produced duplicate flow start: 1 (function operator())
[aiter INFO] 2025-11-20 07:11:33.444 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:282 - post_process_data
abnormal data indices: {162, 131, 227, 260, 292, 6, 73, 46, 433, 343, 24, 314, 191}
[aiter INFO] 2025-11-20 07:11:33.444 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:284 - post_process_data
abnormal data: 490.00599999999986
[aiter INFO] 2025-11-20 07:11:33.444 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:284 - post_process_data
abnormal data: 473.567
[aiter INFO] 2025-11-20 07:11:33.444 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:284 - post_process_data
abnormal data: 493.172
[aiter INFO] 2025-11-20 07:11:33.444 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:284 - post_process_data
abnormal data: 489.8049999999994
[aiter INFO] 2025-11-20 07:11:33.444 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:284 - post_process_data
abnormal data: 477.09600000000046
[aiter INFO] 2025-11-20 07:11:33.444 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:284 - post_process_data
abnormal data: 413.951
[aiter INFO] 2025-11-20 07:11:33.444 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:284 - post_process_data
abnormal data: 411.1850000000002
[aiter INFO] 2025-11-20 07:11:33.445 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:284 - post_process_data
abnormal data: 411.22400000000016
[aiter INFO] 2025-11-20 07:11:33.445 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:284 - post_process_data
abnormal data: 485.875
[aiter INFO] 2025-11-20 07:11:33.445 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:284 - post_process_data
abnormal data: 464.8279999999995
[aiter INFO] 2025-11-20 07:11:33.445 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:284 - post_process_data
abnormal data: 411.1039999999999
[aiter INFO] 2025-11-20 07:11:33.445 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:284 - post_process_data
abnormal data: 466.39100000000053
[aiter INFO] 2025-11-20 07:11:33.445 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:284 - post_process_data
abnormal data: 498.50499999999965
[aiter INFO] 2025-11-20 07:11:33.459 - MainProcess:11766 - /home/memin/code/aiter/aiter/test_common.py:363 - get_trace_perf
                                                       name   cnt  host_time_sum  device_time_sum device_type device_index
0              aiter::fmha_fwd_hd128_bf16_causal_rtna_group  88.0            0.0         39,112.6        CUDA            2
1                                 aiter::fmha_v3_varlen_fwd 100.0       44,685.4              0.0         CPU        11766
[avg us/iter]                                           NaN   NaN          446.9            444.5         NaN          NaN
[aiter INFO] 2025-11-20 07:11:40.492 - MainProcess:11766 - /home/memin/code/aiter/op_tests/test_mha_varlen_loadpt.py:1132 - <module>
mha_varlen summary:
   batch_size  nheads  seqlen_q  seqlen_k    d  d_v  min_seqlen_q  dropout_p  causal  local bias_type  deterministic mha_type           dtype input_layout  fwd_us  fwd_tflops  fwd_gb_per_sec
0           1      32      2048      2048  128  128             1        0.0    True  False        no          False      gqa  torch.bfloat16         BSHD   444.5       298.8           258.6
q_unpad shape is torch.Size([1459, 32, 128]), stride is (6144, 128, 1)
k_unpad shape is torch.Size([5555, 8, 128]), stride is (1024, 128, 1)
v_unpad shape is torch.Size([5555, 8, 128]), stride is (1024, 128, 1)
Output max diff: 0.0078125
Output Pytorch max diff: 0.015625
