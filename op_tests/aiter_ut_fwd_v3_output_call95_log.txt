[aiter INFO] 2025-11-20 06:14:12.100 - MainProcess:10498 - /home/memin/code/aiter/aiter/jit/core.py:133 - get_config_file
merge tuned file under model_configs/ and configs/ /home/memin/code/aiter/aiter/configs/a8w8_bpreshuffle_tuned_gemm.csv:/home/memin/code/aiter/aiter/configs/model_configs/a8w8_bpreshuffle_tuned_gemm_dsv3.csv
[aiter INFO] 2025-11-20 06:14:12.111 - MainProcess:10498 - /home/memin/code/aiter/aiter/jit/core.py:133 - get_config_file
merge tuned file under model_configs/ and configs/ /home/memin/code/aiter/aiter/configs/a8w8_blockscale_tuned_gemm.csv:/home/memin/code/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_ds_v3.csv:/home/memin/code/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_qwen3_235b.csv
[aiter INFO] 2025-11-20 06:14:12.119 - MainProcess:10498 - /home/memin/code/aiter/aiter/jit/core.py:133 - get_config_file
merge tuned file under model_configs/ and configs/ /home/memin/code/aiter/aiter/configs/tuned_fmoe.csv:/home/memin/code/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[aiter INFO] 2025-11-20 06:14:12.133 - MainProcess:10498 - /home/memin/code/aiter/aiter/jit/core.py:133 - get_config_file
merge tuned file under model_configs/ and configs/ /home/memin/code/aiter/aiter/configs/a8w8_blockscale_bpreshuffle_tuned_gemm.csv:/home/memin/code/aiter/aiter/configs/model_configs/a8w8_blockscale_bpreshuffle_tuned_gemm_dsv3.csv
[aiter INFO] 2025-11-20 06:14:12.420 - MainProcess:10498 - /home/memin/code/aiter/aiter/jit/core.py:410 - get_module_custom_op
import [module_aiter_enum] under /home/memin/code/aiter/aiter/jit/module_aiter_enum.so
[aiter INFO] 2025-11-20 06:14:12.421 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling _ActivationType(dummy                        = 0)
[aiter INFO] 2025-11-20 06:14:12.421 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling _QuantType(dummy                        = 0)
[aiter INFO] 2025-11-20 06:14:12.582 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling flash_attn_varlen_func_benchmark(batch_size                   = 1,
                                         nheads                       = 32,
                                         seqlen_q                     = 1459,
                                         seqlen_k                     = 5555,
                                         d                            = 128,
                                         d_v                          = 128,
                                         min_seqlen_q                 = 1,
                                         dropout_p                    = 0.0,
                                         causal                       = True,
                                         local                        = False,
                                         bias_type                    = no,
                                         deterministic                = False,
                                         mha_type                     = gqa,
                                         dtype                        = torch.bfloat16,
                                         input_layout                 = BSHD)
[aiter INFO] 2025-11-20 06:14:13.261 - MainProcess:10498 - /home/memin/code/aiter/aiter/jit/core.py:410 - get_module_custom_op
import [module_fmha_v3_varlen_fwd] under /home/memin/code/aiter/aiter/jit/module_fmha_v3_varlen_fwd.so
[aiter WARNING] 2025-11-20 06:14:13.263 - MainProcess:10498 - /home/memin/code/aiter/aiter/jit/core.py:860 - check_args
type hints mismatch, override to --> fmha_v3_varlen_fwd(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, cu_seqlens_k: Optional[torch.Tensor], max_seqlen_q: int, max_seqlen_k: int, min_seqlen_q: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, how_v3_bf16_cvt: int, out: Optional[torch.Tensor] = None, block_table: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> List[torch.Tensor]
[aiter INFO] 2025-11-20 06:14:13.263 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.264 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.264 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.265 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.266 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.266 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.267 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.267 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.268 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.268 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.269 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.270 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.270 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.271 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.271 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.272 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.272 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.273 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.273 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.274 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.274 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.275 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.276 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.276 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.277 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.277 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.278 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.278 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.279 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.280 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.280 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.281 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.281 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.282 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.282 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.283 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.283 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.284 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.284 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.285 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.285 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.286 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.287 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.287 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.288 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.288 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.289 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.289 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.290 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.290 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.291 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.291 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.292 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.292 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.293 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.293 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.294 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.295 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.295 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.296 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.296 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.297 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.297 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.298 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.298 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.299 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.299 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.300 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.300 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.301 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.301 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.302 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.302 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.303 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.304 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.304 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.305 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.305 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.306 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.306 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.307 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.307 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.308 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.308 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.309 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.309 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.310 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.310 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.311 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.311 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.312 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.313 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.313 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.314 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.314 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.315 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.315 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.316 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.316 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.317 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.317 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.318 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.318 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.319 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:58 - wrapper
avg: 521.5824706719654 us/iter from cuda.Event
[aiter INFO] 2025-11-20 06:14:13.433 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.434 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.434 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.434 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.434 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.434 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.434 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.435 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.435 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.435 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.435 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.435 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.435 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.435 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.435 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.435 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.436 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.436 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.436 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.436 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.436 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.436 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.436 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.436 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.436 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.437 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.437 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.437 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.437 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.437 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.437 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.437 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.437 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.437 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.438 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.438 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.438 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.438 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.438 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.438 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.438 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.438 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.438 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.439 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.439 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.439 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.439 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.439 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.439 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.439 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.439 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.439 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.440 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.440 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.440 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.440 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.440 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.440 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.440 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.440 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.440 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.440 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.441 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.441 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.441 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.441 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.441 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.441 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.441 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.441 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.441 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.442 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.442 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.442 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.442 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.442 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.442 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.442 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.442 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.442 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.443 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.443 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.443 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.443 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.443 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.443 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.443 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.443 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.443 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.444 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.444 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.444 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.444 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.444 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.444 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.444 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.444 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.444 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.445 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.445 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:14:13.445 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = True,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[W1120 06:14:13.684576788 collection.cpp:1116] Warning: ROCTracer produced duplicate flow start: 1 (function operator())
[aiter INFO] 2025-11-20 06:14:13.508 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:282 - post_process_data
abnormal data indices: {133, 198, 166, 263, 233, 298, 328, 357, 6, 46, 24, 572, 542}
[aiter INFO] 2025-11-20 06:14:13.508 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:284 - post_process_data
abnormal data: 479.1399999999999
[aiter INFO] 2025-11-20 06:14:13.508 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:284 - post_process_data
abnormal data: 515.9450000000002
[aiter INFO] 2025-11-20 06:14:13.508 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:284 - post_process_data
abnormal data: 516.8669999999997
[aiter INFO] 2025-11-20 06:14:13.508 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:284 - post_process_data
abnormal data: 494.89699999999993
[aiter INFO] 2025-11-20 06:14:13.509 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:284 - post_process_data
abnormal data: 506.3220000000001
[aiter INFO] 2025-11-20 06:14:13.509 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:284 - post_process_data
abnormal data: 474.85000000000036
[aiter INFO] 2025-11-20 06:14:13.509 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:284 - post_process_data
abnormal data: 470.52100000000064
[aiter INFO] 2025-11-20 06:14:13.509 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:284 - post_process_data
abnormal data: 489.52400000000034
[aiter INFO] 2025-11-20 06:14:13.509 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:284 - post_process_data
abnormal data: 412.266
[aiter INFO] 2025-11-20 06:14:13.509 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:284 - post_process_data
abnormal data: 411.94600000000014
[aiter INFO] 2025-11-20 06:14:13.509 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:284 - post_process_data
abnormal data: 413.78999999999996
[aiter INFO] 2025-11-20 06:14:13.509 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:284 - post_process_data
abnormal data: 470.52000000000044
[aiter INFO] 2025-11-20 06:14:13.509 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:284 - post_process_data
abnormal data: 468.5159999999996
[aiter INFO] 2025-11-20 06:14:13.523 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:363 - get_trace_perf
                                                       name   cnt  host_time_sum  device_time_sum device_type device_index
0              aiter::fmha_fwd_hd128_bf16_causal_rtna_group  88.0            0.0         39,287.0        CUDA            2
1                                 aiter::fmha_v3_varlen_fwd 100.0       44,999.9              0.0         CPU        10498
[avg us/iter]                                           NaN   NaN          450.0            446.4         NaN          NaN
[aiter INFO] 2025-11-20 06:14:13.566 - MainProcess:10498 - /home/memin/code/aiter/aiter/jit/core.py:410 - get_module_custom_op
import [module_fmha_v3_varlen_bwd] under /home/memin/code/aiter/aiter/jit/module_fmha_v3_varlen_bwd.so
[aiter WARNING] 2025-11-20 06:14:13.568 - MainProcess:10498 - /home/memin/code/aiter/aiter/jit/core.py:860 - check_args
type hints mismatch, override to --> fmha_v3_varlen_bwd(dout: torch.Tensor, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, out: torch.Tensor, softmax_lse: torch.Tensor, cu_seqlens_q: torch.Tensor, cu_seqlens_k: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, deterministic: bool, is_v3_atomic_fp32: bool, how_v3_bf16_cvt: int, dq: Optional[torch.Tensor] = None, dk: Optional[torch.Tensor] = None, dv: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, rng_state: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None, cu_seqlens_q_padded: Optional[torch.Tensor] = None, cu_seqlens_k_padded: Optional[torch.Tensor] = None) -> List[torch.Tensor]
[aiter INFO] 2025-11-20 06:14:13.568 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d22200000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d21400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d20600000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.596 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf9200000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.598 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d20600000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d21400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d22200000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.600 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.602 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d20600000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d21400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d22200000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.603 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.605 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d20600000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d21400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d22200000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.606 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.608 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d20600000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d21400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d22200000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.609 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.610 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d20600000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d21400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d22200000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.611 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.613 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d20600000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d21400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d22200000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.614 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.615 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d20600000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d21400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d22200000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.616 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.617 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d20600000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d21400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d22200000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.619 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.620 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d20600000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d21400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d22200000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.621 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.622 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d20600000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d21400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d22200000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.623 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.624 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d20600000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d21400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d22200000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.626 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.627 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d20600000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d21400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d22200000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.628 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.629 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d20600000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d21400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d22200000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.631 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.632 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d20600000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d21400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d22200000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.633 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.634 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d20600000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d21400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d22200000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.635 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.637 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d20600000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d21400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d22200000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.638 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.639 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d20600000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d21400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d22200000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.640 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.641 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d20600000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d21400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d22200000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.643 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.644 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d20600000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d21400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d22200000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.645 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.646 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d20600000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d21400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d22200000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.647 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.649 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d20600000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d21400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d22200000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.650 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.651 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d20600000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d21400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d22200000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.652 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.653 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d20600000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d21400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d22200000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.655 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.656 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d20600000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d21400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d22200000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.657 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.658 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d20600000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d21400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d22200000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.659 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.661 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d20600000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d21400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d22200000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.662 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.663 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d20600000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d21400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d22200000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.664 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.665 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d20600000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d21400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d22200000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.667 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.668 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d20600000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d21400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d22200000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.669 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.670 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d20600000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d21400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d22200000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.671 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.673 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d20600000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d21400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d22200000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.674 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.675 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d20600000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d21400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d22200000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.676 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.677 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d20600000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d21400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d22200000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.679 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.680 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d20600000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d21400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d22200000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.681 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.682 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d20600000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d21400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d22200000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.683 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.684 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d20600000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d21400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d22200000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.686 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.687 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d20600000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d21400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d22200000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.688 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.689 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d20600000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d21400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d22200000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.690 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.692 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d20600000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d21400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d22200000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.693 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.694 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d20600000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d21400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d22200000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.695 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.696 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d20600000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d21400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d22200000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.698 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.699 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d20600000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d21400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d22200000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.700 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.701 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d20600000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d21400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d22200000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.702 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.704 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d20600000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d21400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d22200000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.705 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.706 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d20600000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d21400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d22200000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.707 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.708 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d20600000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d21400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d22200000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.710 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.711 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d20600000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d21400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d22200000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.712 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.714 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d20600000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d21400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d22200000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.715 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.716 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d20600000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d21400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d22200000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.717 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.719 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d20600000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d21400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d22200000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.720 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.722 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d20600000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d21400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d22200000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.723 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:58 - wrapper
avg: 1222.9045287217243 us/iter from cuda.Event
[aiter INFO] 2025-11-20 06:14:13.724 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.724 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1cf6ed9800,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf7a3f800,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf8519000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.725 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.725 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1cf6ed9800,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf7a3f800,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf8519000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.726 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.727 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1cf6ed9800,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf7a3f800,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf8519000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.728 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.729 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1cf6ed9800,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf7a3f800,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf8519000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.729 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.730 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1cf6ed9800,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf7a3f800,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf8519000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.731 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.732 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1cf6ed9800,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf7a3f800,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf8519000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.732 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.733 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1cf6ed9800,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf7a3f800,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf8519000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.734 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.735 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1cf6ed9800,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf7a3f800,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf8519000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.735 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.736 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1cf6ed9800,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf7a3f800,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf8519000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.736 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.737 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1cf6ed9800,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf7a3f800,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf8519000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.737 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.738 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1cf6ed9800,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf7a3f800,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf8519000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.738 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.739 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1cf6ed9800,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf7a3f800,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf8519000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.739 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.740 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1cf6ed9800,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf7a3f800,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf8519000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.740 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.741 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1cf6ed9800,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf7a3f800,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf8519000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.741 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.742 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1cf6ed9800,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf7a3f800,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf8519000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.742 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.743 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1cf6ed9800,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf7a3f800,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf8519000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.743 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.743 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1cf6ed9800,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf7a3f800,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf8519000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.744 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.744 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1cf6ed9800,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf7a3f800,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf8519000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.745 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.745 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1cf6ed9800,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf7a3f800,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf8519000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.746 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.746 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1cf6ed9800,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf7a3f800,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf8519000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.746 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.747 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1cf6ed9800,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf7a3f800,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf8519000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.747 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.748 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1cf6ed9800,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf7a3f800,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf8519000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.748 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.749 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1cf6ed9800,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf7a3f800,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf8519000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.749 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.750 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1cf6ed9800,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf7a3f800,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf8519000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.750 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.750 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1cf6ed9800,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf7a3f800,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf8519000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.751 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.751 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1cf6ed9800,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf7a3f800,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf8519000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.752 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.752 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1cf6ed9800,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf7a3f800,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf8519000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.752 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.753 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1cf6ed9800,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf7a3f800,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf8519000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.753 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.754 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1cf6ed9800,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf7a3f800,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf8519000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.754 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.755 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1cf6ed9800,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf7a3f800,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf8519000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.755 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.755 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1cf6ed9800,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf7a3f800,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf8519000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.756 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.756 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1cf6ed9800,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf7a3f800,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf8519000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.757 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.757 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1cf6ed9800,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf7a3f800,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf8519000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.758 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.758 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1cf6ed9800,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf7a3f800,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf8519000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.759 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.759 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1cf6ed9800,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf7a3f800,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf8519000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.759 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.760 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1cf6ed9800,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf7a3f800,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf8519000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.760 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.761 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1cf6ed9800,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf7a3f800,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf8519000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.761 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.762 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1cf6ed9800,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf7a3f800,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf8519000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.762 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.763 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1cf6ed9800,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf7a3f800,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf8519000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.763 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.763 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1cf6ed9800,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf7a3f800,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf8519000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.764 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.764 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1cf6ed9800,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf7a3f800,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf8519000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.765 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.765 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1cf6ed9800,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf7a3f800,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf8519000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.766 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.766 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1cf6ed9800,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf7a3f800,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf8519000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.766 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.767 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1cf6ed9800,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf7a3f800,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf8519000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.767 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.768 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1cf6ed9800,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf7a3f800,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf8519000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.768 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.769 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1cf6ed9800,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf7a3f800,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf8519000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.769 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.769 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1cf6ed9800,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf7a3f800,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf8519000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.770 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.770 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1cf6ed9800,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf7a3f800,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf8519000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.771 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.771 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1cf6ed9800,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf7a3f800,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf8519000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.772 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.772 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1cf6ed9800,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf7a3f800,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf8519000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:13.773 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_bwd(dout                         = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d48c00000,
                           q                            = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f555e000000,
                           k                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d4a800000,
                           v                            = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d49a00000,
                           out                          = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d38000000,
                           softmax_lse                  = torch.Size([32, 1459]) torch.float32 cuda:0 0x7f555c82e000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f555c800200,
                           max_seqlen_q                 = 1459,
                           max_seqlen_k                 = 5555,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           deterministic                = False,
                           is_v3_atomic_fp32            = True,
                           how_v3_bf16_cvt              = 1,
                           dq                           = torch.Size([1459, 32, 128]) torch.bfloat16 cuda:0 0x7f1d19e00000,
                           dk                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1cf6400000,
                           dv                           = torch.Size([5555, 8, 128]) torch.bfloat16 cuda:0 0x7f1d1a966000,
                           alibi_slopes                 = None,
                           rng_state                    = torch.Size([2]) torch.int64 cuda:0 0x7f555c85ba00,
                           gen                          = None,
                           cu_seqlens_q_padded          = None,
                           cu_seqlens_k_padded          = None)
[aiter INFO] 2025-11-20 06:14:14.069 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:282 - post_process_data
abnormal data indices: {34, 11, 111, 114, 115, 27, 30}
[aiter INFO] 2025-11-20 06:14:14.070 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:284 - post_process_data
abnormal data: 851.136
[aiter INFO] 2025-11-20 06:14:14.070 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:284 - post_process_data
abnormal data: 3.6479999999999677
[aiter INFO] 2025-11-20 06:14:14.070 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:284 - post_process_data
abnormal data: 10.261999999999944
[aiter INFO] 2025-11-20 06:14:14.070 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:284 - post_process_data
abnormal data: 20.886999999999944
[aiter INFO] 2025-11-20 06:14:14.070 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:284 - post_process_data
abnormal data: 20.807000000000016
[aiter INFO] 2025-11-20 06:14:14.070 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:284 - post_process_data
abnormal data: 11.98700000000008
[aiter INFO] 2025-11-20 06:14:14.070 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:284 - post_process_data
abnormal data: 12.547000000000025
[aiter INFO] 2025-11-20 06:14:14.114 - MainProcess:10498 - /home/memin/code/aiter/aiter/test_common.py:363 - get_trace_perf
                                                                                                    name   cnt  host_time_sum  device_time_sum device_type device_index
0              _ZN7ck_tile6kentryILi2ENS_25FmhaBwdConvertQGradKernelINS_24BlockFmhaBwdConvertQGradINS... 100.0            0.0          1,083.4        CUDA            2
1              _ZN7ck_tile6kentryILi2ENS_22FmhaBwdOGradDotOKernelINS_21BlockFmhaBwdOGradDotOINS_36Blo... 100.0            0.0          1,237.3        CUDA            2
2              void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, std:... 200.0            0.0          1,465.6        CUDA            2
3              void at::native::reduce_kernel<128, 4, at::native::ReduceOp<c10::BFloat16, at::native:... 200.0            0.0          4,299.2        CUDA            2
4                                               aiter::fmha_bwd_hd128_bf16_causal_br_a32_rtna_pssk_group 100.0            0.0         82,375.2        CUDA            2
5                                                                                            aten::fill_ 201.0        1,477.6              0.0         CPU        10498
6                                                                                              aten::sum 201.0        4,320.0              0.0         CPU        10498
7                                                                              aiter::fmha_v3_varlen_bwd 100.0       84,695.9              0.0         CPU        10498
[avg us/iter]                                                                                        NaN   NaN          904.9            904.6         NaN          NaN
[aiter INFO] 2025-11-20 06:14:25.067 - MainProcess:10498 - /home/memin/code/aiter/op_tests/test_mha_varlen.py:1108 - <module>
mha_varlen summary:
   batch_size  nheads  seqlen_q  seqlen_k    d  d_v  min_seqlen_q  dropout_p  causal  local bias_type  deterministic mha_type           dtype input_layout  fwd_us  fwd_tflops  fwd_gb_per_sec  bwd_us  bwd_tflops  bwd_gb_per_sec
0           1      32      1459      5555  128  128             1        0.0    True  False        no          False      gqa  torch.bfloat16         BSHD   446.4       297.4           257.4   904.6       367.0           254.3
Output max diff: 0.0009765625
Output Pytorch max diff: 0.001953125
dQ max diff: 0.0009765625
dK max diff: 0.0009765625
dV max diff: 0.0009765625
dQ Pytorch max diff: 0.00244140625
dK Pytorch max diff: 0.0029296875
dV Pytorch max diff: 0.001953125
