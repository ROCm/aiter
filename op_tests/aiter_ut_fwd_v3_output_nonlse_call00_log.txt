[aiter INFO] 2025-11-20 06:28:41.262 - MainProcess:10902 - /home/memin/code/aiter/aiter/jit/core.py:133 - get_config_file
merge tuned file under model_configs/ and configs/ /home/memin/code/aiter/aiter/configs/a8w8_bpreshuffle_tuned_gemm.csv:/home/memin/code/aiter/aiter/configs/model_configs/a8w8_bpreshuffle_tuned_gemm_dsv3.csv
[aiter INFO] 2025-11-20 06:28:41.274 - MainProcess:10902 - /home/memin/code/aiter/aiter/jit/core.py:133 - get_config_file
merge tuned file under model_configs/ and configs/ /home/memin/code/aiter/aiter/configs/a8w8_blockscale_tuned_gemm.csv:/home/memin/code/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_ds_v3.csv:/home/memin/code/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_qwen3_235b.csv
[aiter INFO] 2025-11-20 06:28:41.282 - MainProcess:10902 - /home/memin/code/aiter/aiter/jit/core.py:133 - get_config_file
merge tuned file under model_configs/ and configs/ /home/memin/code/aiter/aiter/configs/tuned_fmoe.csv:/home/memin/code/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[aiter INFO] 2025-11-20 06:28:41.295 - MainProcess:10902 - /home/memin/code/aiter/aiter/jit/core.py:133 - get_config_file
merge tuned file under model_configs/ and configs/ /home/memin/code/aiter/aiter/configs/a8w8_blockscale_bpreshuffle_tuned_gemm.csv:/home/memin/code/aiter/aiter/configs/model_configs/a8w8_blockscale_bpreshuffle_tuned_gemm_dsv3.csv
[aiter INFO] 2025-11-20 06:28:41.629 - MainProcess:10902 - /home/memin/code/aiter/aiter/jit/core.py:410 - get_module_custom_op
import [module_aiter_enum] under /home/memin/code/aiter/aiter/jit/module_aiter_enum.so
[aiter INFO] 2025-11-20 06:28:41.630 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling _ActivationType(dummy                        = 0)
[aiter INFO] 2025-11-20 06:28:41.630 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling _QuantType(dummy                        = 0)
[aiter INFO] 2025-11-20 06:28:41.730 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling flash_attn_varlen_func_benchmark(batch_size                   = 1,
                                         nheads                       = 32,
                                         seqlen_q                     = 2048,
                                         seqlen_k                     = 2048,
                                         d                            = 128,
                                         d_v                          = 128,
                                         min_seqlen_q                 = 1,
                                         dropout_p                    = 0.0,
                                         causal                       = True,
                                         local                        = False,
                                         bias_type                    = no,
                                         deterministic                = False,
                                         mha_type                     = gqa,
                                         dtype                        = torch.bfloat16,
                                         input_layout                 = BSHD)
[aiter INFO] 2025-11-20 06:28:42.143 - MainProcess:10902 - /home/memin/code/aiter/aiter/jit/core.py:410 - get_module_custom_op
import [module_fmha_v3_varlen_fwd] under /home/memin/code/aiter/aiter/jit/module_fmha_v3_varlen_fwd.so
[aiter WARNING] 2025-11-20 06:28:42.144 - MainProcess:10902 - /home/memin/code/aiter/aiter/jit/core.py:860 - check_args
type hints mismatch, override to --> fmha_v3_varlen_fwd(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, cu_seqlens_k: Optional[torch.Tensor], max_seqlen_q: int, max_seqlen_k: int, min_seqlen_q: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, how_v3_bf16_cvt: int, out: Optional[torch.Tensor] = None, block_table: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> List[torch.Tensor]
[aiter INFO] 2025-11-20 06:28:42.144 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.145 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.145 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.145 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.146 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.146 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.146 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.147 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.147 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.147 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.147 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.148 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.148 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.148 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.148 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.149 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.149 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.150 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.150 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.150 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.150 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.151 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.151 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.151 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.152 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.152 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.152 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.152 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.153 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.153 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.153 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.153 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.154 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.154 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.154 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.155 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.155 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.155 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.155 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.156 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.156 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.156 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.156 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.157 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.157 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.157 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.158 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.158 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.158 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.158 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.159 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.159 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.159 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.159 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.160 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.160 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.160 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.161 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.161 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.161 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.161 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.162 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.162 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.162 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.162 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.163 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.163 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.163 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.164 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.164 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.164 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.164 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.165 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.165 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.165 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.165 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.166 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.166 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.166 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.167 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.167 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.167 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.167 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.168 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.168 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.168 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.168 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.169 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.169 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.169 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.169 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.170 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.170 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.170 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.170 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.171 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.171 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.171 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.171 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.172 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.172 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.172 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.172 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.173 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:58 - wrapper
avg: 255.8492084245871 us/iter from cuda.Event
[aiter INFO] 2025-11-20 06:28:42.287 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.287 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.288 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.288 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.288 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.288 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.288 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.288 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.288 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.289 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.289 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.289 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.289 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.289 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.289 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.289 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.289 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.289 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.290 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.290 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.290 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.290 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.290 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.290 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.290 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.290 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.291 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.291 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.291 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.291 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.291 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.291 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.291 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.291 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.291 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.292 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.292 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.292 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.292 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.292 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.292 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.292 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.292 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.293 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.293 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.293 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.293 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.293 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.293 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.293 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.293 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.294 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.294 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.294 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.294 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.294 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.294 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.294 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.294 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.294 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.295 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.295 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.295 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.295 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.295 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.295 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.295 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.295 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.296 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.296 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.296 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.296 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.296 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.296 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.296 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.296 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.296 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.297 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.297 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.297 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.297 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.297 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.297 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.297 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.297 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.298 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.298 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.298 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.298 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.298 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.298 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.298 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.298 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.299 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.299 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.299 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.299 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.299 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.299 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.299 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[aiter INFO] 2025-11-20 06:28:42.299 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:214 - log_args

calling fmha_v3_varlen_fwd(q                            = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f82d1600000,
                           k                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aaf800000,
                           v                            = torch.Size([2048, 8, 128]) torch.bfloat16 cuda:0 0x7f5aafc00000,
                           cu_seqlens_q                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400000,
                           cu_seqlens_k                 = torch.Size([2]) torch.int32 cuda:0 0x7f5aaf400200,
                           max_seqlen_q                 = 2048,
                           max_seqlen_k                 = 2048,
                           min_seqlen_q                 = 1,
                           dropout_p                    = 0.0,
                           softmax_scale                = 0.08838834764831845,
                           logits_soft_cap              = 0.0,
                           zero_tensors                 = False,
                           is_causal                    = True,
                           window_size_left             = -1,
                           window_size_right            = -1,
                           return_softmax_lse           = False,
                           return_dropout_randval       = False,
                           how_v3_bf16_cvt              = 1,
                           out                          = torch.Size([2048, 32, 128]) torch.bfloat16 cuda:0 0x7f5aac800000,
                           block_table                  = None,
                           bias                         = None,
                           alibi_slopes                 = None,
                           gen                          = None)
[W1120 06:28:42.509474210 collection.cpp:1116] Warning: ROCTracer produced duplicate flow start: 1 (function operator())
[aiter INFO] 2025-11-20 06:28:42.332 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:282 - post_process_data
abnormal data indices: {393, 6}
[aiter INFO] 2025-11-20 06:28:42.332 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:284 - post_process_data
abnormal data: 173.558
[aiter INFO] 2025-11-20 06:28:42.332 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:284 - post_process_data
abnormal data: 143.45900000000006
[aiter INFO] 2025-11-20 06:28:42.346 - MainProcess:10902 - /home/memin/code/aiter/aiter/test_common.py:363 - get_trace_perf
                                                       name   cnt  host_time_sum  device_time_sum device_type device_index
0              aiter::fmha_fwd_hd128_bf16_causal_rtna_group  99.0            0.0         13,653.9        CUDA            2
1                                 aiter::fmha_v3_varlen_fwd 100.0       13,827.5              0.0         CPU        10902
[avg us/iter]                                           NaN   NaN          138.3            137.9         NaN          NaN
[aiter INFO] 2025-11-20 06:28:53.428 - MainProcess:10902 - /home/memin/code/aiter/op_tests/test_mha_varlen.py:1116 - <module>
mha_varlen summary:
   batch_size  nheads  seqlen_q  seqlen_k    d  d_v  min_seqlen_q  dropout_p  causal  local bias_type  deterministic mha_type           dtype input_layout  fwd_us  fwd_tflops  fwd_gb_per_sec
0           1      32      2048      2048  128  128             1        0.0    True  False        no          False      gqa  torch.bfloat16         BSHD   137.9       498.3           486.6
Output max diff: 0.0078125
Output Pytorch max diff: 0.015625
