#blocked = #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [1, 64], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [16, 1], threadsPerWarp = [8, 8], warpsPerCTA = [1, 4], order = [0, 1]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 16], threadsPerWarp = [8, 8], warpsPerCTA = [4, 1], order = [1, 0]}>
#linear = #ttg.linear<{register = [[0, 64], [0, 128], [1, 0], [2, 0]], lane = [[0, 2], [0, 4], [0, 8], [0, 16], [0, 32], [0, 1]], warp = [[0, 0], [0, 0]], block = []}>
#linear1 = #ttg.linear<{register = [[0, 64], [0, 128], [4, 0]], lane = [[0, 2], [0, 4], [0, 8], [0, 16], [0, 32], [0, 1]], warp = [[1, 0], [2, 0]], block = []}>
#linear2 = #ttg.linear<{register = [[0, 0, 1, 0, 0, 0, 0], [0, 0, 2, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0], [2, 0, 0, 0, 0, 0, 0]], lane = [[0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 2, 0, 0, 0], [0, 0, 0, 4, 0, 0, 0], [0, 0, 0, 8, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0]], warp = [[0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0]], block = []}>
#linear3 = #ttg.linear<{register = [[0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 2, 0], [1, 0, 0, 0, 0, 0, 0], [2, 0, 0, 0, 0, 0, 0]], lane = [[0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 2, 0, 0, 0], [0, 0, 0, 4, 0, 0, 0], [0, 0, 0, 8, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0]], warp = [[0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0]], block = []}>
#linear4 = #ttg.linear<{register = [[0, 1], [0, 2], [32, 0], [64, 0]], lane = [[0, 4], [1, 0], [2, 0], [4, 0], [8, 0], [16, 0]], warp = [[0, 0], [0, 0]], block = []}>
#linear5 = #ttg.linear<{register = [[0, 0, 1, 0, 0, 0, 0], [0, 0, 2, 0, 0, 0, 0], [4, 0, 0, 0, 0, 0, 0]], lane = [[0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 2, 0, 0, 0], [0, 0, 0, 4, 0, 0, 0], [0, 0, 0, 8, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0]], warp = [[1, 0, 0, 0, 0, 0, 0], [2, 0, 0, 0, 0, 0, 0]], block = []}>
#linear6 = #ttg.linear<{register = [[0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 2, 0], [4, 0, 0, 0, 0, 0, 0]], lane = [[0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 2, 0, 0, 0], [0, 0, 0, 4, 0, 0, 0], [0, 0, 0, 8, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0]], warp = [[1, 0, 0, 0, 0, 0, 0], [2, 0, 0, 0, 0, 0, 0]], block = []}>
#linear7 = #ttg.linear<{register = [[0, 1], [0, 2], [128, 0]], lane = [[0, 4], [1, 0], [2, 0], [4, 0], [8, 0], [16, 0]], warp = [[32, 0], [64, 0]], block = []}>
#loc = loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":177:0)
#mma = #ttg.amd_mfma<{versionMajor = 4, versionMinor = 0, warpsPerCTA = [1, 4], tilesPerWarp = [2, 2], instrShape = [16, 16], isTransposed = true}>
#shared = #ttg.swizzled_shared<{vec = 16, perPhase = 2, maxPhase = 8, order = [1, 0]}>
#shared1 = #ttg.swizzled_shared<{vec = 16, perPhase = 2, maxPhase = 8, order = [0, 1]}>
#shared2 = #ttg.swizzled_shared<{vec = 16, perPhase = 1, maxPhase = 1, order = [1, 0]}>
#smem = #ttg.shared_memory
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.target = "hip:gfx950", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @_gemm_afp4_wfp4_kernel_preshuffled_scales(%arg0: !tt.ptr<i8> {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32} loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":177:0), %arg1: !tt.ptr<i8> {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32} loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":177:0), %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32} loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":177:0), %arg3: !tt.ptr<i8> {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32} loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":177:0), %arg4: !tt.ptr<i8> {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32} loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":177:0), %arg5: i32 {tt.divisibility = 16 : i32} loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":177:0), %arg6: i32 {tt.divisibility = 16 : i32} loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":177:0), %arg7: i32 {tt.divisibility = 16 : i32} loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":177:0), %arg8: i32 {tt.divisibility = 16 : i32} loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":177:0), %arg9: i32 {tt.divisibility = 16 : i32} loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":177:0), %arg10: i32 {tt.divisibility = 16 : i32} loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":177:0), %arg11: i32 {tt.divisibility = 16 : i32} loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":177:0), %arg12: i32 {tt.divisibility = 16 : i32} loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":177:0), %arg13: i32 {tt.divisibility = 16 : i32} loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":177:0)) attributes {noinline = false} {
    %c0_i32 = arith.constant 0 : i32 loc(#loc1)
    %cst = arith.constant dense<0.000000e+00> : tensor<128x256xf32, #mma> loc(#loc1)
    %c4_i32 = arith.constant 4 : i32 loc(#loc1)
    %c8_i32 = arith.constant 8 : i32 loc(#loc1)
    %c256_i32 = arith.constant 256 : i32 loc(#loc1)
    %c128_i32 = arith.constant 128 : i32 loc(#loc1)
    %true = arith.constant true loc(#loc1)
    %c255_i32 = arith.constant 255 : i32 loc(#loc1)
    %c1_i32 = arith.constant 1 : i32 loc(#loc1)
    %cst_0 = arith.constant dense<true> : tensor<8x256xi1, #blocked> loc(#loc1)
    %c62_i32 = arith.constant 62 : i32 loc(#loc1)
    %cst_1 = arith.constant dense<true> : tensor<128x256xi1, #blocked1> loc(#loc1)
    %cst_2 = arith.constant dense<true> : tensor<128x128xi1, #blocked2> loc(#loc1)
    %cst_3 = arith.constant dense<true> : tensor<4x256xi1, #blocked> loc(#loc1)
    %c3_i32 = arith.constant 3 : i32 loc(#loc1)
    llvm.intr.assume %true : i1 loc(#loc2)
    llvm.intr.assume %true : i1 loc(#loc3)
    llvm.intr.assume %true : i1 loc(#loc4)
    llvm.intr.assume %true : i1 loc(#loc5)
    llvm.intr.assume %true : i1 loc(#loc6)
    llvm.intr.assume %true : i1 loc(#loc7)
    llvm.intr.assume %true : i1 loc(#loc8)
    llvm.intr.assume %true : i1 loc(#loc9)
    llvm.intr.assume %true : i1 loc(#loc10)
    llvm.intr.assume %true : i1 loc(#loc11)
    %0 = tt.get_program_id x : i32 loc(#loc12)
    %1 = arith.addi %arg6, %c255_i32 : i32 loc(#loc76)
    %2 = arith.divsi %1, %c256_i32 : i32 loc(#loc77)
    %3 = arith.divsi %0, %2 : i32 loc(#loc78)
    %4 = arith.remsi %0, %2 : i32 loc(#loc79)
    llvm.intr.assume %true : i1 loc(#loc19)
    llvm.intr.assume %true : i1 loc(#loc20)
    %5 = arith.cmpi sgt, %arg7, %c0_i32 : i32 loc(#loc21)
    scf.if %5 {
      %6 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked2}>> loc(#loc23)
      %7 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked1}>> loc(#loc23)
      %8 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc23)
      %9 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 0, parent = #blocked2}>> loc(#loc23)
      %10 = arith.muli %3, %c128_i32 : i32 loc(#loc24)
      %11 = tt.splat %10 : i32 -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked2}>> loc(#loc25)
      %12 = arith.addi %11, %6 : tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked2}>> loc(#loc25)
      %13 = tt.splat %arg5 : i32 -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked2}>> loc(#loc26)
      %14 = arith.remsi %12, %13 : tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked2}>> loc(#loc26)
      %15 = arith.muli %4, %c256_i32 : i32 loc(#loc27)
      %16 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32, #ttg.slice<{dim = 0, parent = #blocked}>> loc(#loc28)
      %17 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32, #ttg.slice<{dim = 0, parent = #mma}>> loc(#loc28)
      %18 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32, #ttg.slice<{dim = 0, parent = #blocked1}>> loc(#loc28)
      %19 = tt.splat %15 : i32 -> tensor<256xi32, #ttg.slice<{dim = 0, parent = #blocked1}>> loc(#loc29)
      %20 = arith.addi %19, %18 : tensor<256xi32, #ttg.slice<{dim = 0, parent = #blocked1}>> loc(#loc29)
      %21 = tt.splat %arg6 : i32 -> tensor<256xi32, #ttg.slice<{dim = 0, parent = #blocked1}>> loc(#loc30)
      %22 = arith.remsi %20, %21 : tensor<256xi32, #ttg.slice<{dim = 0, parent = #blocked1}>> loc(#loc30)
      %23 = tt.expand_dims %14 {axis = 1 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked2}>> -> tensor<128x1xi32, #blocked2> loc(#loc31)
      %24 = tt.splat %arg8 : i32 -> tensor<128x1xi32, #blocked2> loc(#loc31)
      %25 = arith.muli %23, %24 : tensor<128x1xi32, #blocked2> loc(#loc31)
      %26 = tt.broadcast %25 : tensor<128x1xi32, #blocked2> -> tensor<128x128xi32, #blocked2> loc(#loc31)
      %27 = tt.expand_dims %9 {axis = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 0, parent = #blocked2}>> -> tensor<1x128xi32, #blocked2> loc(#loc31)
      %28 = tt.broadcast %27 : tensor<1x128xi32, #blocked2> -> tensor<128x128xi32, #blocked2> loc(#loc31)
      %29 = arith.addi %26, %28 : tensor<128x128xi32, #blocked2> loc(#loc31)
      %30 = tt.expand_dims %7 {axis = 1 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked1}>> -> tensor<128x1xi32, #blocked1> loc(#loc32)
      %31 = tt.broadcast %30 : tensor<128x1xi32, #blocked1> -> tensor<128x256xi32, #blocked1> loc(#loc32)
      %32 = tt.expand_dims %22 {axis = 0 : i32} : tensor<256xi32, #ttg.slice<{dim = 0, parent = #blocked1}>> -> tensor<1x256xi32, #blocked1> loc(#loc32)
      %33 = tt.splat %arg9 : i32 -> tensor<1x256xi32, #blocked1> loc(#loc32)
      %34 = arith.muli %32, %33 : tensor<1x256xi32, #blocked1> loc(#loc32)
      %35 = tt.broadcast %34 : tensor<1x256xi32, #blocked1> -> tensor<128x256xi32, #blocked1> loc(#loc32)
      %36 = arith.addi %31, %35 : tensor<128x256xi32, #blocked1> loc(#loc32)
      %37 = arith.muli %4, %c8_i32 : i32 loc(#loc33)
      %38 = tt.make_range {end = 8 : i32, start = 0 : i32} : tensor<8xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc34)
      %39 = tt.splat %37 : i32 -> tensor<8xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc35)
      %40 = arith.addi %39, %38 : tensor<8xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc35)
      %41 = tt.splat %arg6 : i32 -> tensor<8xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc36)
      %42 = arith.remsi %40, %41 : tensor<8xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc36)
      %43 = tt.expand_dims %42 {axis = 1 : i32} : tensor<8xi32, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<8x1xi32, #blocked> loc(#loc37)
      %44 = tt.splat %arg13 : i32 -> tensor<8x1xi32, #blocked> loc(#loc37)
      %45 = arith.muli %43, %44 : tensor<8x1xi32, #blocked> loc(#loc37)
      %46 = tt.broadcast %45 : tensor<8x1xi32, #blocked> -> tensor<8x256xi32, #blocked> loc(#loc38)
      %47 = tt.expand_dims %16 {axis = 0 : i32} : tensor<256xi32, #ttg.slice<{dim = 0, parent = #blocked}>> -> tensor<1x256xi32, #blocked> loc(#loc38)
      %48 = tt.broadcast %47 : tensor<1x256xi32, #blocked> -> tensor<8x256xi32, #blocked> loc(#loc38)
      %49 = arith.addi %48, %46 : tensor<8x256xi32, #blocked> loc(#loc38)
      %50 = arith.muli %3, %c4_i32 : i32 loc(#loc39)
      %51 = tt.make_range {end = 4 : i32, start = 0 : i32} : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc40)
      %52 = tt.splat %50 : i32 -> tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc41)
      %53 = arith.addi %52, %51 : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc41)
      %54 = tt.splat %arg5 : i32 -> tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc42)
      %55 = arith.remsi %53, %54 : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc42)
      %56 = tt.expand_dims %55 {axis = 1 : i32} : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<4x1xi32, #blocked> loc(#loc43)
      %57 = tt.splat %arg12 : i32 -> tensor<4x1xi32, #blocked> loc(#loc43)
      %58 = arith.muli %56, %57 : tensor<4x1xi32, #blocked> loc(#loc43)
      %59 = tt.broadcast %58 : tensor<4x1xi32, #blocked> -> tensor<4x256xi32, #blocked> loc(#loc44)
      %60 = tt.broadcast %47 : tensor<1x256xi32, #blocked> -> tensor<4x256xi32, #blocked> loc(#loc44)
      %61 = arith.addi %60, %59 : tensor<4x256xi32, #blocked> loc(#loc44)
      %62 = ttg.local_alloc : () -> !ttg.memdesc<3x128x128xi8, #shared, #smem, mutable> loc(#loc45)
      %63 = ttg.local_alloc : () -> !ttg.memdesc<3x128x256xi8, #shared1, #smem, mutable> loc(#loc46)
      %64 = ttg.local_alloc : () -> !ttg.memdesc<3x4x256xi8, #shared2, #smem, mutable> loc(#loc47)
      %65 = ttg.local_alloc : () -> !ttg.memdesc<3x8x256xi8, #shared2, #smem, mutable> loc(#loc48)
      %66 = ttg.memdesc_subview %64[%c0_i32, %c0_i32, %c0_i32] : !ttg.memdesc<3x4x256xi8, #shared2, #smem, mutable> -> !ttg.memdesc<4x256xi8, #shared2, #smem, mutable> loc(#loc47)
      %67 = amdgpu.buffer_load_to_local %arg3[%61] mask = %cst_3 stride = %arg12 into %66 : <i8>[tensor<4x256xi32, #blocked>]  -> <4x256xi8, #shared2, #smem, mutable> loc(#loc47)
      %68 = ttg.async_commit_group %67 loc(#loc47)
      %69 = ttg.memdesc_subview %65[%c0_i32, %c0_i32, %c0_i32] : !ttg.memdesc<3x8x256xi8, #shared2, #smem, mutable> -> !ttg.memdesc<8x256xi8, #shared2, #smem, mutable> loc(#loc48)
      %70 = amdgpu.buffer_load_to_local %arg4[%49] mask = %cst_0 stride = %arg13 cacheModifier = cg into %69 : <i8>[tensor<8x256xi32, #blocked>]  -> <8x256xi8, #shared2, #smem, mutable> loc(#loc48)
      %71 = ttg.async_commit_group %70 loc(#loc48)
      %72 = ttg.memdesc_subview %62[%c0_i32, %c0_i32, %c0_i32] : !ttg.memdesc<3x128x128xi8, #shared, #smem, mutable> -> !ttg.memdesc<128x128xi8, #shared, #smem, mutable> loc(#loc45)
      %73 = amdgpu.buffer_load_to_local %arg0[%29] mask = %cst_2 stride = %arg8 into %72 : <i8>[tensor<128x128xi32, #blocked2>]  -> <128x128xi8, #shared, #smem, mutable> loc(#loc45)
      %74 = ttg.async_commit_group %73 loc(#loc45)
      %75 = ttg.memdesc_subview %63[%c0_i32, %c0_i32, %c0_i32] : !ttg.memdesc<3x128x256xi8, #shared1, #smem, mutable> -> !ttg.memdesc<128x256xi8, #shared1, #smem, mutable> loc(#loc46)
      %76 = amdgpu.buffer_load_to_local %arg1[%36] mask = %cst_1 stride = %arg9 cacheModifier = cg into %75 : <i8>[tensor<128x256xi32, #blocked1>]  -> <128x256xi8, #shared1, #smem, mutable> loc(#loc46)
      %77 = ttg.async_commit_group %76 loc(#loc46)
      %78 = tt.addptr %arg0, %c128_i32 : !tt.ptr<i8>, i32 loc(#loc49)
      %79 = tt.addptr %arg1, %c128_i32 : !tt.ptr<i8>, i32 loc(#loc50)
      %80 = tt.addptr %arg3, %c256_i32 : !tt.ptr<i8>, i32 loc(#loc51)
      %81 = tt.addptr %arg4, %c256_i32 : !tt.ptr<i8>, i32 loc(#loc52)
      %82 = ttg.memdesc_subview %64[%c1_i32, %c0_i32, %c0_i32] : !ttg.memdesc<3x4x256xi8, #shared2, #smem, mutable> -> !ttg.memdesc<4x256xi8, #shared2, #smem, mutable> loc(#loc47)
      %83 = amdgpu.buffer_load_to_local %80[%61] mask = %cst_3 stride = %arg12 into %82 : <i8>[tensor<4x256xi32, #blocked>]  -> <4x256xi8, #shared2, #smem, mutable> loc(#loc47)
      %84 = ttg.async_commit_group %83 loc(#loc47)
      %85 = ttg.memdesc_subview %65[%c1_i32, %c0_i32, %c0_i32] : !ttg.memdesc<3x8x256xi8, #shared2, #smem, mutable> -> !ttg.memdesc<8x256xi8, #shared2, #smem, mutable> loc(#loc48)
      %86 = amdgpu.buffer_load_to_local %81[%49] mask = %cst_0 stride = %arg13 cacheModifier = cg into %85 : <i8>[tensor<8x256xi32, #blocked>]  -> <8x256xi8, #shared2, #smem, mutable> loc(#loc48)
      %87 = ttg.async_commit_group %86 loc(#loc48)
      %88 = ttg.memdesc_subview %62[%c1_i32, %c0_i32, %c0_i32] : !ttg.memdesc<3x128x128xi8, #shared, #smem, mutable> -> !ttg.memdesc<128x128xi8, #shared, #smem, mutable> loc(#loc45)
      %89 = amdgpu.buffer_load_to_local %78[%29] mask = %cst_2 stride = %arg8 into %88 : <i8>[tensor<128x128xi32, #blocked2>]  -> <128x128xi8, #shared, #smem, mutable> loc(#loc45)
      %90 = ttg.async_commit_group %89 loc(#loc45)
      %91 = ttg.memdesc_subview %63[%c1_i32, %c0_i32, %c0_i32] : !ttg.memdesc<3x128x256xi8, #shared1, #smem, mutable> -> !ttg.memdesc<128x256xi8, #shared1, #smem, mutable> loc(#loc46)
      %92 = amdgpu.buffer_load_to_local %79[%36] mask = %cst_1 stride = %arg9 cacheModifier = cg into %91 : <i8>[tensor<128x256xi32, #blocked1>]  -> <128x256xi8, #shared1, #smem, mutable> loc(#loc46)
      %93 = ttg.async_commit_group %92 loc(#loc46)
      %94:22 = scf.for %arg14 = %c0_i32 to %c62_i32 step %c1_i32 iter_args(%arg15 = %cst, %arg16 = %80, %arg17 = %78, %arg18 = %79, %arg19 = %c1_i32, %arg20 = %68, %arg21 = %84, %arg22 = %71, %arg23 = %87, %arg24 = %74, %arg25 = %90, %arg26 = %77, %arg27 = %93, %arg28 = %66, %arg29 = %82, %arg30 = %69, %arg31 = %85, %arg32 = %72, %arg33 = %88, %arg34 = %75, %arg35 = %91, %arg36 = %81) -> (tensor<128x256xf32, #mma>, !tt.ptr<i8>, !tt.ptr<i8>, !tt.ptr<i8>, i32, !ttg.async.token, !ttg.async.token, !ttg.async.token, !ttg.async.token, !ttg.async.token, !ttg.async.token, !ttg.async.token, !ttg.async.token, !ttg.memdesc<4x256xi8, #shared2, #smem, mutable>, !ttg.memdesc<4x256xi8, #shared2, #smem, mutable>, !ttg.memdesc<8x256xi8, #shared2, #smem, mutable>, !ttg.memdesc<8x256xi8, #shared2, #smem, mutable>, !ttg.memdesc<128x128xi8, #shared, #smem, mutable>, !ttg.memdesc<128x128xi8, #shared, #smem, mutable>, !ttg.memdesc<128x256xi8, #shared1, #smem, mutable>, !ttg.memdesc<128x256xi8, #shared1, #smem, mutable>, !tt.ptr<i8>)  : i32 {
        %151 = ttg.async_wait %arg20, %arg22, %arg24, %arg26 {num = 15 : i32} loc(#loc47)
        %152 = tt.addptr %arg17, %c128_i32 : !tt.ptr<i8>, i32 loc(#loc49)
        %153 = tt.addptr %arg18, %c128_i32 : !tt.ptr<i8>, i32 loc(#loc50)
        %154 = tt.addptr %arg16, %c256_i32 : !tt.ptr<i8>, i32 loc(#loc51)
        %155 = tt.addptr %arg36, %c256_i32 : !tt.ptr<i8>, i32 loc(#loc52)
        %156 = arith.addi %arg19, %c1_i32 : i32 loc(#loc53)
        %157 = arith.cmpi slt, %156, %c3_i32 : i32 loc(#loc53)
        %158 = arith.select %157, %156, %c0_i32 : i32 loc(#loc53)
        %159 = ttg.memdesc_subview %64[%158, %c0_i32, %c0_i32] : !ttg.memdesc<3x4x256xi8, #shared2, #smem, mutable> -> !ttg.memdesc<4x256xi8, #shared2, #smem, mutable> loc(#loc47)
        %160 = amdgpu.buffer_load_to_local %154[%61] stride = %arg12 into %159 : <i8>[tensor<4x256xi32, #blocked>]  -> <4x256xi8, #shared2, #smem, mutable> loc(#loc47)
        %161 = ttg.async_commit_group %160 loc(#loc47)
        %162 = ttg.local_load %arg28 token %151 : !ttg.memdesc<4x256xi8, #shared2, #smem, mutable> -> tensor<4x256xi8, #linear> loc(#loc54)
        %163 = ttg.memdesc_subview %65[%158, %c0_i32, %c0_i32] : !ttg.memdesc<3x8x256xi8, #shared2, #smem, mutable> -> !ttg.memdesc<8x256xi8, #shared2, #smem, mutable> loc(#loc48)
        %164 = amdgpu.buffer_load_to_local %155[%49] stride = %arg13 cacheModifier = cg into %163 : <i8>[tensor<8x256xi32, #blocked>]  -> <8x256xi8, #shared2, #smem, mutable> loc(#loc48)
        %165 = ttg.async_commit_group %164 loc(#loc48)
        %166 = ttg.local_load %arg30 token %151 : !ttg.memdesc<8x256xi8, #shared2, #smem, mutable> -> tensor<8x256xi8, #linear1> loc(#loc55)
        %167 = tt.reshape %162 : tensor<4x256xi8, #linear> -> tensor<4x1x4x16x2x2x1xi8, #linear2> loc(#loc56)
        %168 = tt.trans %167 {order = array<i32: 0, 1, 5, 3, 4, 2, 6>} : tensor<4x1x4x16x2x2x1xi8, #linear2> -> tensor<4x1x2x16x2x4x1xi8, #linear3> loc(#loc57)
        %169 = tt.reshape %168 : tensor<4x1x2x16x2x4x1xi8, #linear3> -> tensor<128x8xi8, #linear4> loc(#loc54)
        %170 = tt.reshape %166 : tensor<8x256xi8, #linear1> -> tensor<8x1x4x16x2x2x1xi8, #linear5> loc(#loc58)
        %171 = tt.trans %170 {order = array<i32: 0, 1, 5, 3, 4, 2, 6>} : tensor<8x1x4x16x2x2x1xi8, #linear5> -> tensor<8x1x2x16x2x4x1xi8, #linear6> loc(#loc59)
        %172 = tt.reshape %171 : tensor<8x1x2x16x2x4x1xi8, #linear6> -> tensor<256x8xi8, #linear7> loc(#loc55)
        %173 = ttg.memdesc_subview %62[%158, %c0_i32, %c0_i32] : !ttg.memdesc<3x128x128xi8, #shared, #smem, mutable> -> !ttg.memdesc<128x128xi8, #shared, #smem, mutable> loc(#loc45)
        %174 = amdgpu.buffer_load_to_local %152[%29] stride = %arg8 into %173 : <i8>[tensor<128x128xi32, #blocked2>]  -> <128x128xi8, #shared, #smem, mutable> loc(#loc45)
        %175 = ttg.async_commit_group %174 loc(#loc45)
        %176 = ttg.local_load %arg32 token %151 : !ttg.memdesc<128x128xi8, #shared, #smem, mutable> -> tensor<128x128xi8, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 16}>> loc(#loc45)
        %177 = ttg.memdesc_subview %63[%158, %c0_i32, %c0_i32] : !ttg.memdesc<3x128x256xi8, #shared1, #smem, mutable> -> !ttg.memdesc<128x256xi8, #shared1, #smem, mutable> loc(#loc46)
        %178 = amdgpu.buffer_load_to_local %153[%36] stride = %arg9 cacheModifier = cg into %177 : <i8>[tensor<128x256xi32, #blocked1>]  -> <128x256xi8, #shared1, #smem, mutable> loc(#loc46)
        %179 = ttg.async_commit_group %178 loc(#loc46)
        %180 = ttg.local_load %arg34 token %151 : !ttg.memdesc<128x256xi8, #shared1, #smem, mutable> -> tensor<128x256xi8, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 16}>> loc(#loc46)
        %181 = tt.dot_scaled %176 scale %169, %180 scale %172, %arg15 lhs = e2m1 rhs = e2m1 {fastMath = false} : tensor<128x128xi8, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 16}>>, tensor<128x8xi8, #linear4> * tensor<128x256xi8, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 16}>>, tensor<256x8xi8, #linear7> -> tensor<128x256xf32, #mma> loc(#loc60)
        scf.yield %181, %154, %152, %153, %158, %arg21, %161, %arg23, %165, %arg25, %175, %arg27, %179, %arg29, %159, %arg31, %163, %arg33, %173, %arg35, %177, %155 : tensor<128x256xf32, #mma>, !tt.ptr<i8>, !tt.ptr<i8>, !tt.ptr<i8>, i32, !ttg.async.token, !ttg.async.token, !ttg.async.token, !ttg.async.token, !ttg.async.token, !ttg.async.token, !ttg.async.token, !ttg.async.token, !ttg.memdesc<4x256xi8, #shared2, #smem, mutable>, !ttg.memdesc<4x256xi8, #shared2, #smem, mutable>, !ttg.memdesc<8x256xi8, #shared2, #smem, mutable>, !ttg.memdesc<8x256xi8, #shared2, #smem, mutable>, !ttg.memdesc<128x128xi8, #shared, #smem, mutable>, !ttg.memdesc<128x128xi8, #shared, #smem, mutable>, !ttg.memdesc<128x256xi8, #shared1, #smem, mutable>, !ttg.memdesc<128x256xi8, #shared1, #smem, mutable>, !tt.ptr<i8> loc(#loc53)
      } loc(#loc53)
      %95 = ttg.async_wait %94#5, %94#7, %94#9, %94#11, %94#6, %94#8, %94#10, %94#12 {num = 0 : i32} loc(#loc47)
      %96 = ttg.local_load %94#13 token %95 : !ttg.memdesc<4x256xi8, #shared2, #smem, mutable> -> tensor<4x256xi8, #linear> loc(#loc54)
      %97 = ttg.local_load %94#15 token %95 : !ttg.memdesc<8x256xi8, #shared2, #smem, mutable> -> tensor<8x256xi8, #linear1> loc(#loc55)
      %98 = tt.reshape %96 : tensor<4x256xi8, #linear> -> tensor<4x1x4x16x2x2x1xi8, #linear2> loc(#loc56)
      %99 = tt.trans %98 {order = array<i32: 0, 1, 5, 3, 4, 2, 6>} : tensor<4x1x4x16x2x2x1xi8, #linear2> -> tensor<4x1x2x16x2x4x1xi8, #linear3> loc(#loc57)
      %100 = tt.reshape %99 : tensor<4x1x2x16x2x4x1xi8, #linear3> -> tensor<128x8xi8, #linear4> loc(#loc54)
      %101 = tt.reshape %97 : tensor<8x256xi8, #linear1> -> tensor<8x1x4x16x2x2x1xi8, #linear5> loc(#loc58)
      %102 = tt.trans %101 {order = array<i32: 0, 1, 5, 3, 4, 2, 6>} : tensor<8x1x4x16x2x2x1xi8, #linear5> -> tensor<8x1x2x16x2x4x1xi8, #linear6> loc(#loc59)
      %103 = tt.reshape %102 : tensor<8x1x2x16x2x4x1xi8, #linear6> -> tensor<256x8xi8, #linear7> loc(#loc55)
      %104 = ttg.local_load %94#17 token %95 : !ttg.memdesc<128x128xi8, #shared, #smem, mutable> -> tensor<128x128xi8, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 16}>> loc(#loc45)
      %105 = ttg.local_load %94#19 token %95 : !ttg.memdesc<128x256xi8, #shared1, #smem, mutable> -> tensor<128x256xi8, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 16}>> loc(#loc46)
      %106 = tt.dot_scaled %104 scale %100, %105 scale %103, %94#0 lhs = e2m1 rhs = e2m1 {fastMath = false} : tensor<128x128xi8, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 16}>>, tensor<128x8xi8, #linear4> * tensor<128x256xi8, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 16}>>, tensor<256x8xi8, #linear7> -> tensor<128x256xf32, #mma> loc(#loc60)
      %107 = ttg.local_load %94#14 token %95 : !ttg.memdesc<4x256xi8, #shared2, #smem, mutable> -> tensor<4x256xi8, #linear> loc(#loc54)
      %108 = ttg.local_load %94#16 token %95 : !ttg.memdesc<8x256xi8, #shared2, #smem, mutable> -> tensor<8x256xi8, #linear1> loc(#loc55)
      %109 = tt.reshape %107 : tensor<4x256xi8, #linear> -> tensor<4x1x4x16x2x2x1xi8, #linear2> loc(#loc56)
      %110 = tt.trans %109 {order = array<i32: 0, 1, 5, 3, 4, 2, 6>} : tensor<4x1x4x16x2x2x1xi8, #linear2> -> tensor<4x1x2x16x2x4x1xi8, #linear3> loc(#loc57)
      %111 = tt.reshape %110 : tensor<4x1x2x16x2x4x1xi8, #linear3> -> tensor<128x8xi8, #linear4> loc(#loc54)
      %112 = tt.reshape %108 : tensor<8x256xi8, #linear1> -> tensor<8x1x4x16x2x2x1xi8, #linear5> loc(#loc58)
      %113 = tt.trans %112 {order = array<i32: 0, 1, 5, 3, 4, 2, 6>} : tensor<8x1x4x16x2x2x1xi8, #linear5> -> tensor<8x1x2x16x2x4x1xi8, #linear6> loc(#loc59)
      %114 = tt.reshape %113 : tensor<8x1x2x16x2x4x1xi8, #linear6> -> tensor<256x8xi8, #linear7> loc(#loc55)
      %115 = ttg.local_load %94#18 token %95 : !ttg.memdesc<128x128xi8, #shared, #smem, mutable> -> tensor<128x128xi8, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 16}>> loc(#loc45)
      %116 = ttg.local_load %94#20 token %95 : !ttg.memdesc<128x256xi8, #shared1, #smem, mutable> -> tensor<128x256xi8, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 16}>> loc(#loc46)
      %117 = tt.dot_scaled %115 scale %111, %116 scale %114, %106 lhs = e2m1 rhs = e2m1 {fastMath = false} : tensor<128x128xi8, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 16}>>, tensor<128x8xi8, #linear4> * tensor<128x256xi8, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 16}>>, tensor<256x8xi8, #linear7> -> tensor<128x256xf32, #mma> loc(#loc60)
      ttg.local_dealloc %62 : !ttg.memdesc<3x128x128xi8, #shared, #smem, mutable> loc(#loc53)
      ttg.local_dealloc %63 : !ttg.memdesc<3x128x256xi8, #shared1, #smem, mutable> loc(#loc53)
      ttg.local_dealloc %64 : !ttg.memdesc<3x4x256xi8, #shared2, #smem, mutable> loc(#loc53)
      ttg.local_dealloc %65 : !ttg.memdesc<3x8x256xi8, #shared2, #smem, mutable> loc(#loc53)
      %118 = arith.truncf %117 : tensor<128x256xf32, #mma> to tensor<128x256xbf16, #mma> loc(#loc61)
      %119 = arith.extsi %8 : tensor<128xi32, #ttg.slice<{dim = 1, parent = #mma}>> to tensor<128xi64, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc62)
      %120 = arith.extsi %10 : i32 to i64 loc(#loc63)
      %121 = tt.splat %120 : i64 -> tensor<128xi64, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc63)
      %122 = arith.addi %121, %119 : tensor<128xi64, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc63)
      %123 = arith.extsi %17 : tensor<256xi32, #ttg.slice<{dim = 0, parent = #mma}>> to tensor<256xi64, #ttg.slice<{dim = 0, parent = #mma}>> loc(#loc64)
      %124 = arith.extsi %15 : i32 to i64 loc(#loc65)
      %125 = tt.splat %124 : i64 -> tensor<256xi64, #ttg.slice<{dim = 0, parent = #mma}>> loc(#loc65)
      %126 = arith.addi %125, %123 : tensor<256xi64, #ttg.slice<{dim = 0, parent = #mma}>> loc(#loc65)
      %127 = tt.expand_dims %122 {axis = 1 : i32} : tensor<128xi64, #ttg.slice<{dim = 1, parent = #mma}>> -> tensor<128x1xi64, #mma> loc(#loc66)
      %128 = arith.extsi %arg11 : i32 to i64 loc(#loc67)
      %129 = tt.expand_dims %119 {axis = 1 : i32} : tensor<128xi64, #ttg.slice<{dim = 1, parent = #mma}>> -> tensor<128x1xi64, #mma> loc(#loc68)
      %130 = arith.muli %128, %120 : i64 loc(#loc68)
      %131 = tt.splat %128 : i64 -> tensor<128x1xi64, #mma> loc(#loc68)
      %132 = arith.muli %131, %129 : tensor<128x1xi64, #mma> loc(#loc68)
      %133 = tt.addptr %arg2, %130 : !tt.ptr<bf16>, i64 loc(#loc68)
      %134 = arith.trunci %132 : tensor<128x1xi64, #mma> to tensor<128x1xi32, #mma> loc(#loc68)
      %135 = tt.expand_dims %126 {axis = 0 : i32} : tensor<256xi64, #ttg.slice<{dim = 0, parent = #mma}>> -> tensor<1x256xi64, #mma> loc(#loc69)
      %136 = tt.broadcast %134 : tensor<128x1xi32, #mma> -> tensor<128x256xi32, #mma> loc(#loc70)
      %137 = tt.expand_dims %123 {axis = 0 : i32} : tensor<256xi64, #ttg.slice<{dim = 0, parent = #mma}>> -> tensor<1x256xi64, #mma> loc(#loc70)
      %138 = tt.broadcast %137 : tensor<1x256xi64, #mma> -> tensor<128x256xi64, #mma> loc(#loc70)
      %139 = tt.addptr %133, %124 : !tt.ptr<bf16>, i64 loc(#loc70)
      %140 = arith.trunci %138 : tensor<128x256xi64, #mma> to tensor<128x256xi32, #mma> loc(#loc70)
      %141 = arith.addi %140, %136 : tensor<128x256xi32, #mma> loc(#loc70)
      %142 = arith.extsi %arg5 : i32 to i64 loc(#loc71)
      %143 = tt.splat %142 : i64 -> tensor<128x1xi64, #mma> loc(#loc71)
      %144 = arith.cmpi slt, %127, %143 : tensor<128x1xi64, #mma> loc(#loc71)
      %145 = arith.extsi %arg6 : i32 to i64 loc(#loc72)
      %146 = tt.splat %145 : i64 -> tensor<1x256xi64, #mma> loc(#loc72)
      %147 = arith.cmpi slt, %135, %146 : tensor<1x256xi64, #mma> loc(#loc72)
      %148 = tt.broadcast %144 : tensor<128x1xi1, #mma> -> tensor<128x256xi1, #mma> loc(#loc73)
      %149 = tt.broadcast %147 : tensor<1x256xi1, #mma> -> tensor<128x256xi1, #mma> loc(#loc73)
      %150 = arith.andi %148, %149 : tensor<128x256xi1, #mma> loc(#loc73)
      amdgpu.buffer_store %118, %139[%141], %150 cacheModifier = wt : tensor<128x256xbf16, #mma> loc(#loc74)
    } loc(#loc22)
    tt.return loc(#loc75)
  } loc(#loc)
} loc(#loc)
#loc1 = loc(unknown)
#loc2 = loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":214:14)
#loc3 = loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":215:14)
#loc4 = loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":216:14)
#loc5 = loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":217:14)
#loc6 = loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":218:14)
#loc7 = loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":219:14)
#loc8 = loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":220:14)
#loc9 = loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":221:14)
#loc10 = loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":222:14)
#loc11 = loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":223:14)
#loc12 = loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":228:32)
#loc13 = loc("/app/OAI-triton/python/triton/language/standard.py":40:22)
#loc14 = loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":232:27)
#loc15 = loc("/app/OAI-triton/python/triton/language/standard.py":40:28)
#loc16 = loc("/app/aiter/aiter/ops/triton/utils/pid_preprocessing.py":51:23)
#loc17 = loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":237:48)
#loc18 = loc("/app/aiter/aiter/ops/triton/utils/pid_preprocessing.py":52:22)
#loc19 = loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":242:14)
#loc20 = loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":243:14)
#loc21 = loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":247:42)
#loc22 = loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":247:7)
#loc23 = loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":253:30)
#loc24 = loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":255:27)
#loc25 = loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":255:42)
#loc26 = loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":255:72)
#loc27 = loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":256:27)
#loc28 = loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":256:55)
#loc29 = loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":256:42)
#loc30 = loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":256:72)
#loc31 = loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":258:12)
#loc32 = loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":261:12)
#loc33 = loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":266:21)
#loc34 = loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":266:57)
#loc35 = loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":266:43)
#loc36 = loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":267:12)
#loc37 = loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":274:14)
#loc38 = loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":275:14)
#loc39 = loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":289:25)
#loc40 = loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":289:61)
#loc41 = loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":289:47)
#loc42 = loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":290:16)
#loc43 = loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":293:18)
#loc44 = loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":294:18)
#loc45 = loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":321:28)
#loc46 = loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":322:28)
#loc47 = loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":304:31)
#loc48 = loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":305:31)
#loc49 = loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":334:22)
#loc50 = loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":335:22)
#loc51 = loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":339:32)
#loc52 = loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":340:28)
#loc53 = loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":303:43)
#loc54 = loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":310:117)
#loc55 = loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":314:113)
#loc56 = loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":310:71)
#loc57 = loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":310:92)
#loc58 = loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":314:67)
#loc59 = loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":314:88)
#loc60 = loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":331:75)
#loc61 = loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":342:27)
#loc62 = loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":345:71)
#loc63 = loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":345:41)
#loc64 = loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":346:71)
#loc65 = loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":346:41)
#loc66 = loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":349:34)
#loc67 = loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":349:26)
#loc68 = loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":349:14)
#loc69 = loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":350:34)
#loc70 = loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":350:14)
#loc71 = loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":353:37)
#loc72 = loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":353:62)
#loc73 = loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":353:43)
#loc74 = loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":354:25)
#loc75 = loc("/app/aiter/aiter/ops/triton/gemm_afp4wfp4.py":247:4)
#loc76 = loc(callsite(#loc13 at #loc14))
#loc77 = loc(callsite(#loc15 at #loc14))
#loc78 = loc(callsite(#loc16 at #loc17))
#loc79 = loc(callsite(#loc18 at #loc17))
