// SPDX-License-Identifier: MIT
// Copyright (c) 2018-2024, Advanced Micro Devices, Inc. All rights reserved.

// auto generated by generate.py
#include "ck_tile/ops/fmha/block/variants.hpp"
#include "fmha_fwd_sparge.hpp"
#include "block_fmha_pipeline_qr_ks_vs_async_sparge.hpp"
#include "fmha_fwd_sparge_kernel.hpp"


#include <cstdio>

#include <hip/hip_runtime.h>

namespace {
bool get_num_cus(unsigned& num_cus) {
    int device;
    auto status = hipGetDevice(&device);
    if(status != hipSuccess) {
        fprintf(stderr, "failed to get device");
        return false;
    }

    hipDeviceProp_t props{};
    status = hipGetDeviceProperties(&props, device);
    if(status != hipSuccess) {
        fprintf(stderr, "failed to get device properties");
        return false;
    }

    num_cus = props.multiProcessorCount;
    return true;
}

unsigned get_num_thread_blocks(unsigned batch, unsigned nheads, unsigned max_seqlen_q, unsigned kM0) {
    const unsigned num_m_blocks = (max_seqlen_q + kM0 - 1) / kM0;
    const unsigned num_n_blocks = 1; // we assume that num_n_blocks is always 1

    return batch * nheads * num_m_blocks * num_n_blocks;
}
} // namespace

float fmha_sparge_fwd(fmha_sparge_fwd_traits t, fmha_sparge_fwd_args a, const ck_tile::stream_config& s){
    float r = -1;

    [[maybe_unused]] const float min_cu_util_rate = 0.8; // minimum CU utilization rate

    unsigned num_cus;
    if (!get_num_cus(num_cus)) {
        return r;
    }

    [[maybe_unused]] auto get_num_blocks = [&](unsigned kM0) {
        return get_num_thread_blocks(a.batch, a.nhead_q, a.max_seqlen_q, kM0);
    };
    
    const bool has_load_tr = ck_tile::is_load_tr_supported();

    if(has_load_tr){
    if(t.data_type.compare("fp16") == 0){
        if (t.hdim_q <= 32 && t.hdim_v <= 32) {

        }
        else if (t.hdim_q <= 64 && t.hdim_v <= 64) {

        }
        else if (t.hdim_q <= 96 && t.hdim_v <= 128) {

        }
        else if (t.hdim_q <= 128 && t.hdim_v <= 128) {

        }
        else if (t.hdim_q <= 192 && t.hdim_v <= 128) {

        }
        else if (t.hdim_q <= 192 && t.hdim_v <= 192) {

        }

    }
    else if(t.data_type.compare("bf16") == 0){
        if (t.hdim_q <= 32 && t.hdim_v <= 32) {

        }
        else if (t.hdim_q <= 64 && t.hdim_v <= 64) {

        }
        else if (t.hdim_q <= 96 && t.hdim_v <= 128) {

        }
        else if (t.hdim_q <= 128 && t.hdim_v <= 128) {

        }
        else if (t.hdim_q <= 192 && t.hdim_v <= 128) {

        }
        else if (t.hdim_q <= 192 && t.hdim_v <= 192) {

        }

    }

    }
    if(true){
    if(t.data_type.compare("fp16") == 0){
        if (t.hdim_q <= 32 && t.hdim_v <= 32) {
            if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdFp16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }

        }
        else if (t.hdim_q <= 64 && t.hdim_v <= 64) {
            if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdFp16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }

        }
        else if (t.hdim_q <= 96 && t.hdim_v <= 128) {
            if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }

        }
        else if (t.hdim_q <= 128 && t.hdim_v <= 128) {
            if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }

        }
        else if (t.hdim_q <= 192 && t.hdim_v <= 128) {
            if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, true, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }

        }
        else if (t.hdim_q <= 192 && t.hdim_v <= 192) {
            if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdFp16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }

        }

    }
    else if(t.data_type.compare("bf16") == 0){
        if (t.hdim_q <= 32 && t.hdim_v <= 32) {
            if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, false, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<32, FmhaFwdBf16, true, 128, 64, 16, 32, 32, 32, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }

        }
        else if (t.hdim_q <= 64 && t.hdim_v <= 64) {
            if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 64 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 64 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, false, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<64, FmhaFwdBf16, true, 128, 64, 32, 64, 32, 64, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }

        }
        else if (t.hdim_q <= 96 && t.hdim_v <= 128) {
            if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<96, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 96, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }

        }
        else if (t.hdim_q <= 128 && t.hdim_v <= 128) {
            if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<128, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 128, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }

        }
        else if (t.hdim_q <= 192 && t.hdim_v <= 128) {
            if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, true, 128, 128, 32, 128, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }

        }
        else if (t.hdim_q <= 192 && t.hdim_v <= 192) {
            if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, false, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k != 0 && a.seqlen_k % 128 == 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, false, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == false) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true) && (true/*fall back to largest tile*/) && (a.seqlen_k == 0 || a.seqlen_k % 128 != 0) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, false, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == true) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, true, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type == mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<false>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::no_bias) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::NO_BIAS, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == true)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, true, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == true) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, true, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == true) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, true>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }
            else if((t.is_group_mode == true) && (t.is_v_rowmajor == true) && (t.has_logits_soft_cap == false) && (t.mask_type != mask_enum::no_mask) && (t.bias_type == bias_enum::alibi) && (t.has_lse == false)  && (t.has_dropout == false) && (t.do_fp8_static_quant == false) && (t.skip_min_seqlen_q == false) &&
                        (true/*group mode spad always true*/) && (true/*fall back to largest tile*/) && (true/*group mode skpad always true*/) && (a.hdim_q % 8 == 0) && (a.hdim_v % 8 == 0) && ((true) && (true))) {
                using trait_ = fmha_sparge_fwd_traits_<192, FmhaFwdBf16, true, 128, 128, 32, 192, 32, 192, true, ck_tile::BlockFmhaPipelineEnum::QRKSVS_ASYNC, false, ck_tile::SimplifiedGenericAttentionMask<true>, ck_tile::BlockAttentionBiasEnum::ALIBI, false, false, false, true, true, true, true, false, false>;
                return fmha_sparge_fwd_<trait_>(s, a);
            }

        }

    }

    }

    return r;
}
